{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LEGION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LEGION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LEGION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_data = pd.read_csv('train.csv')\n",
    "val_data = pd.read_csv('val.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "lengthTestData = len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine text data for preprocessing\n",
    "text = pd.concat([train_data['text'], val_data['text']], ignore_index=True)\n",
    "text_test = test_data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "linguistic_features = pd.concat([train_data.iloc[:,-9:], val_data.iloc[:,-9:]], ignore_index=True)\n",
    "test_linguistic_features = test_data.iloc[:,-9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words_per_Sentence</th>\n",
       "      <th>Percentage_Questions</th>\n",
       "      <th>Percentage_First_Person_Singular</th>\n",
       "      <th>Percentage_Second_Person</th>\n",
       "      <th>Percentage_Third_Person</th>\n",
       "      <th>Percentage_Negation</th>\n",
       "      <th>Percentage_Exclusive</th>\n",
       "      <th>Percentage_Causation</th>\n",
       "      <th>Percentage_Sense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.930502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.702703</td>\n",
       "      <td>5.405405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.735562</td>\n",
       "      <td>0.607903</td>\n",
       "      <td>1.519757</td>\n",
       "      <td>1.519757</td>\n",
       "      <td>4.863222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.416667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.191781</td>\n",
       "      <td>0.547945</td>\n",
       "      <td>0.821918</td>\n",
       "      <td>1.095890</td>\n",
       "      <td>6.027397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.254545</td>\n",
       "      <td>3.636364</td>\n",
       "      <td>0.085543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.790419</td>\n",
       "      <td>1.197605</td>\n",
       "      <td>1.710864</td>\n",
       "      <td>2.138580</td>\n",
       "      <td>3.079555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>3.181818</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>1.636364</td>\n",
       "      <td>2.181818</td>\n",
       "      <td>6.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54714</th>\n",
       "      <td>56.166667</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>0.296736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.154303</td>\n",
       "      <td>1.186944</td>\n",
       "      <td>1.186944</td>\n",
       "      <td>0.593472</td>\n",
       "      <td>5.341246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54715</th>\n",
       "      <td>22.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.183406</td>\n",
       "      <td>0.436681</td>\n",
       "      <td>0.873362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.240175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54716</th>\n",
       "      <td>18.095238</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>1.842105</td>\n",
       "      <td>3.684211</td>\n",
       "      <td>5.263158</td>\n",
       "      <td>1.052632</td>\n",
       "      <td>2.105263</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>5.789474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54717</th>\n",
       "      <td>16.611354</td>\n",
       "      <td>10.043668</td>\n",
       "      <td>1.156677</td>\n",
       "      <td>2.576236</td>\n",
       "      <td>2.970557</td>\n",
       "      <td>1.445846</td>\n",
       "      <td>2.602524</td>\n",
       "      <td>1.393270</td>\n",
       "      <td>2.628812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54718</th>\n",
       "      <td>28.111111</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>0.988142</td>\n",
       "      <td>0.592885</td>\n",
       "      <td>4.743083</td>\n",
       "      <td>1.185771</td>\n",
       "      <td>1.185771</td>\n",
       "      <td>1.383399</td>\n",
       "      <td>4.743083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54719 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Words_per_Sentence  Percentage_Questions  \\\n",
       "0               28.777778              0.000000   \n",
       "1               32.900000              0.000000   \n",
       "2               30.416667              0.000000   \n",
       "3               21.254545              3.636364   \n",
       "4               24.444444              0.000000   \n",
       "...                   ...                   ...   \n",
       "54714           56.166667             16.666667   \n",
       "54715           22.900000              0.000000   \n",
       "54716           18.095238             14.285714   \n",
       "54717           16.611354             10.043668   \n",
       "54718           28.111111             11.111111   \n",
       "\n",
       "       Percentage_First_Person_Singular  Percentage_Second_Person  \\\n",
       "0                              0.000000                  0.000000   \n",
       "1                              0.000000                  0.000000   \n",
       "2                              0.000000                  0.000000   \n",
       "3                              0.085543                  0.000000   \n",
       "4                              0.000000                  0.090909   \n",
       "...                                 ...                       ...   \n",
       "54714                          0.296736                  0.000000   \n",
       "54715                          0.000000                  0.000000   \n",
       "54716                          1.842105                  3.684211   \n",
       "54717                          1.156677                  2.576236   \n",
       "54718                          0.988142                  0.592885   \n",
       "\n",
       "       Percentage_Third_Person  Percentage_Negation  Percentage_Exclusive  \\\n",
       "0                     1.930502             0.000000              0.000000   \n",
       "1                     2.735562             0.607903              1.519757   \n",
       "2                     2.191781             0.547945              0.821918   \n",
       "3                     4.790419             1.197605              1.710864   \n",
       "4                     3.181818             0.545455              1.636364   \n",
       "...                        ...                  ...                   ...   \n",
       "54714                 4.154303             1.186944              1.186944   \n",
       "54715                 2.183406             0.436681              0.873362   \n",
       "54716                 5.263158             1.052632              2.105263   \n",
       "54717                 2.970557             1.445846              2.602524   \n",
       "54718                 4.743083             1.185771              1.185771   \n",
       "\n",
       "       Percentage_Causation  Percentage_Sense  \n",
       "0                  2.702703          5.405405  \n",
       "1                  1.519757          4.863222  \n",
       "2                  1.095890          6.027397  \n",
       "3                  2.138580          3.079555  \n",
       "4                  2.181818          6.181818  \n",
       "...                     ...               ...  \n",
       "54714              0.593472          5.341246  \n",
       "54715              0.000000          5.240175  \n",
       "54716              1.315789          5.789474  \n",
       "54717              1.393270          2.628812  \n",
       "54718              1.383399          4.743083  \n",
       "\n",
       "[54719 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linguistic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words_per_Sentence</th>\n",
       "      <th>Percentage_Questions</th>\n",
       "      <th>Percentage_First_Person_Singular</th>\n",
       "      <th>Percentage_Second_Person</th>\n",
       "      <th>Percentage_Third_Person</th>\n",
       "      <th>Percentage_Negation</th>\n",
       "      <th>Percentage_Exclusive</th>\n",
       "      <th>Percentage_Causation</th>\n",
       "      <th>Percentage_Sense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.265957</td>\n",
       "      <td>2.393617</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.595745</td>\n",
       "      <td>2.393617</td>\n",
       "      <td>7.446809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>2.142857</td>\n",
       "      <td>2.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.369863</td>\n",
       "      <td>1.369863</td>\n",
       "      <td>1.369863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.219178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.466667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.404531</td>\n",
       "      <td>0.242718</td>\n",
       "      <td>3.559871</td>\n",
       "      <td>0.485437</td>\n",
       "      <td>0.566343</td>\n",
       "      <td>1.779935</td>\n",
       "      <td>3.478964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.278689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>3.278689</td>\n",
       "      <td>6.557377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6076</th>\n",
       "      <td>26.708333</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>0.468019</td>\n",
       "      <td>0.624025</td>\n",
       "      <td>4.680187</td>\n",
       "      <td>0.936037</td>\n",
       "      <td>2.184087</td>\n",
       "      <td>1.560062</td>\n",
       "      <td>1.872075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6077</th>\n",
       "      <td>32.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.834862</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.305810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.587156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6078</th>\n",
       "      <td>25.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.427083</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>2.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6079</th>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>2.380952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6080</th>\n",
       "      <td>17.125000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.729927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.029197</td>\n",
       "      <td>0.729927</td>\n",
       "      <td>5.109489</td>\n",
       "      <td>2.919708</td>\n",
       "      <td>5.109489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6081 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Words_per_Sentence  Percentage_Questions  \\\n",
       "0              25.066667              0.000000   \n",
       "1              23.333333              0.000000   \n",
       "2              24.333333              0.000000   \n",
       "3              27.466667              0.000000   \n",
       "4              40.666667              0.000000   \n",
       "...                  ...                   ...   \n",
       "6076           26.708333              8.333333   \n",
       "6077           32.700000              0.000000   \n",
       "6078           25.600000              0.000000   \n",
       "6079           63.000000              0.000000   \n",
       "6080           17.125000             25.000000   \n",
       "\n",
       "      Percentage_First_Person_Singular  Percentage_Second_Person  \\\n",
       "0                             0.000000                  0.265957   \n",
       "1                             0.000000                  0.000000   \n",
       "2                             0.000000                  0.000000   \n",
       "3                             0.404531                  0.242718   \n",
       "4                             0.000000                  0.000000   \n",
       "...                                ...                       ...   \n",
       "6076                          0.468019                  0.624025   \n",
       "6077                          0.000000                  0.000000   \n",
       "6078                          0.000000                  0.000000   \n",
       "6079                          0.000000                  0.000000   \n",
       "6080                          0.729927                  0.000000   \n",
       "\n",
       "      Percentage_Third_Person  Percentage_Negation  Percentage_Exclusive  \\\n",
       "0                    2.393617             0.000000              1.595745   \n",
       "1                    2.857143             2.857143              2.857143   \n",
       "2                    1.369863             1.369863              1.369863   \n",
       "3                    3.559871             0.485437              0.566343   \n",
       "4                    3.278689             0.000000              0.819672   \n",
       "...                       ...                  ...                   ...   \n",
       "6076                 4.680187             0.936037              2.184087   \n",
       "6077                 1.834862             0.000000              0.305810   \n",
       "6078                 4.427083             0.520833              0.781250   \n",
       "6079                 5.555556             0.000000              2.777778   \n",
       "6080                 8.029197             0.729927              5.109489   \n",
       "\n",
       "      Percentage_Causation  Percentage_Sense  \n",
       "0                 2.393617          7.446809  \n",
       "1                 2.142857          2.857143  \n",
       "2                 0.000000          8.219178  \n",
       "3                 1.779935          3.478964  \n",
       "4                 3.278689          6.557377  \n",
       "...                    ...               ...  \n",
       "6076              1.560062          1.872075  \n",
       "6077              0.000000          4.587156  \n",
       "6078              3.125000          2.083333  \n",
       "6079              0.793651          2.380952  \n",
       "6080              2.919708          5.109489  \n",
       "\n",
       "[6081 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_linguistic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenization and lowercase\n",
    "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]  # Remove stopwords and lemmatize\n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessed = text.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        beirut ( reuters ) - iran military chief met s...\n",
      "1        hanoi ( reuters ) - top u.s. envoy began two-d...\n",
      "2        ( reuters ) - four u.s. senator asked senate j...\n",
      "3        first read morning briefing meet press nbc pol...\n",
      "4        cairo ( reuters ) - six month egypt election ,...\n",
      "                               ...                        \n",
      "54714    lack oversight prof donald trump totally unfit...\n",
      "54715    tucker carlson responded espn anchor calling p...\n",
      "54716    getting something nothing rage president profe...\n",
      "54717    black emanuelle fixed 1976. attila speaking eu...\n",
      "54718    chaos broke legal american illegal alien clash...\n",
      "Name: text, Length: 54719, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        beirut (reuters) - iran s military chief met w...\n",
      "1        hanoi (reuters) - a top u.s. envoy began a two...\n",
      "2        (reuters) - four u.s. senators have asked the ...\n",
      "3        first read is a morning briefing from meet the...\n",
      "4        cairo (reuters) - six months before egypt s el...\n",
      "                               ...                        \n",
      "54714    this lack of oversight proves that donald trum...\n",
      "54715    tucker carlson responded to an espn anchor cal...\n",
      "54716    because getting something for nothing is all t...\n",
      "54717    black emanuelle fixed all that in 1976. attila...\n",
      "54718    chaos broke out after legal americans and ille...\n",
      "Name: text, Length: 54719, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF vectorizer without specifying max_features\n",
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the preprocessed text data\n",
    "count_matrix = count_vectorizer.fit_transform(text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169079\n"
     ]
    }
   ],
   "source": [
    "# Get the number of unique tokens\n",
    "num_unique_tokens = len(count_vectorizer.get_feature_names_out())\n",
    "print(num_unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize TF-IDF vectorizer with the determined max_features\n",
    "count_vectorizer = CountVectorizer(max_features=num_unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit and transform the text data again with the updated max_features\n",
    "count_matrix = count_vectorizer.fit_transform(text_preprocessed)\n",
    "\n",
    "# Convert the TF-IDF matrix to a CSR (Compressed Sparse Row) matrix for efficient row-wise operations\n",
    "csr_count_matrix = csr_matrix(count_matrix)\n",
    "\n",
    "# Find the row index with the maximum number of filled values\n",
    "max_features_row_index = csr_count_matrix.getnnz(axis=1).argmax()\n",
    "\n",
    "# Get the number of features in the document with the most filled values\n",
    "max_features = csr_count_matrix[max_features_row_index].count_nonzero()\n",
    "\n",
    "svd = TruncatedSVD(n_components=int(max_features*0.3))\n",
    "count_matrix = svd.fit_transform(count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dense_tfidf_matrix = tfidf_matrix[:len(train_data)]\n",
    "#dense_val_tfidf_matrix = tfidf_matrix[len(train_data):len(train_data) + len(val_data)]\n",
    "\n",
    "# Merging the Validation and Training Data into one for a larger training dataset.\n",
    "#dense_count_matrix = count_matrix[:len(train_data) + len(val_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate linguistic features and TF-IDF matrix horizontally\n",
    "dense_count_with_linguistic = hstack([count_matrix, csr_matrix(linguistic_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert them into Arrays\n",
    "train_labels = train_data['label'].values\n",
    "val_labels = val_data['label'].values\n",
    "\n",
    "dense_labels = np.concatenate((train_data['label'].values, val_data['label'].values), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the SVM model\n",
    "svm_model = SVC(kernel='rbf', C=5.0)\n",
    "#svm_model = SVC(kernel='poly', C=20.0, degree=2, coef0=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base estimator (Decision Tree) with max depth 5\n",
    "base_estimator = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "# Create an AdaBoost classifier with custom settings\n",
    "adaboost_model = AdaBoostClassifier(\n",
    "    estimator=base_estimator,  # Using the custom decision tree as the base estimator\n",
    "    n_estimators=115,  # Increasing the number of estimators to 700\n",
    "    learning_rate=0.5,  # Lowering the learning rate to 0.3\n",
    "    algorithm='SAMME.R'  # Using 'SAMME.R' algorithm for real probability estimates\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the GBM model\n",
    "gbm_model = GradientBoostingClassifier(learning_rate=0.5, n_estimators=200, loss='exponential')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the LR model\n",
    "log_reg_model = LogisticRegression(max_iter=1000, penalty='l2', multi_class='multinomial') # Initialize Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the MLP model\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(600, 10), max_iter=300, activation='relu', solver='adam', learning_rate='adaptive', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of tuples where each tuple contains a name for the model and the trained model\n",
    "models = [\n",
    "    ('SVM', svm_model),\n",
    "    ('Logistic Regression', log_reg_model),\n",
    "    ('Gradient Boosting', gbm_model),\n",
    "    ('AdaBoost', adaboost_model),\n",
    "    ('MLPClassifier', mlp_model)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a VotingClassifier instance\n",
    "voting_classifier = VotingClassifier(estimators=models, voting='hard', verbose=True)  # Use 'soft' voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Voting] ...................... (1 of 5) Processing SVM, total=36.7min\n",
      "[Voting] ...... (2 of 5) Processing Logistic Regression, total= 4.4min\n",
      "[Voting] ....... (3 of 5) Processing Gradient Boosting, total=209.6min\n",
      "[Voting] ................ (4 of 5) Processing AdaBoost, total=205.9min\n",
      "Iteration 1, loss = 0.19295569\n",
      "Iteration 2, loss = 0.10158637\n",
      "Iteration 3, loss = 0.06861961\n",
      "Iteration 4, loss = 0.04499214\n",
      "Iteration 5, loss = 0.03048995\n",
      "Iteration 6, loss = 0.02721402\n",
      "Iteration 7, loss = 0.01655278\n",
      "Iteration 8, loss = 0.01521542\n",
      "Iteration 9, loss = 0.01765217\n",
      "Iteration 10, loss = 0.01202516\n",
      "Iteration 11, loss = 0.00764982\n",
      "Iteration 12, loss = 0.01114141\n",
      "Iteration 13, loss = 0.00676641\n",
      "Iteration 14, loss = 0.00868804\n",
      "Iteration 15, loss = 0.01508351\n",
      "Iteration 16, loss = 0.01069626\n",
      "Iteration 17, loss = 0.00612658\n",
      "Iteration 18, loss = 0.00842267\n",
      "Iteration 19, loss = 0.00513715\n",
      "Iteration 20, loss = 0.00289721\n",
      "Iteration 21, loss = 0.00504551\n",
      "Iteration 22, loss = 0.00304401\n",
      "Iteration 23, loss = 0.01111901\n",
      "Iteration 24, loss = 0.00504682\n",
      "Iteration 25, loss = 0.00299820\n",
      "Iteration 26, loss = 0.00548167\n",
      "Iteration 27, loss = 0.00678546\n",
      "Iteration 28, loss = 0.00914687\n",
      "Iteration 29, loss = 0.00523288\n",
      "Iteration 30, loss = 0.00360424\n",
      "Iteration 31, loss = 0.00244216\n",
      "Iteration 32, loss = 0.00184676\n",
      "Iteration 33, loss = 0.00217758\n",
      "Iteration 34, loss = 0.00173234\n",
      "Iteration 35, loss = 0.00163323\n",
      "Iteration 36, loss = 0.00148746\n",
      "Iteration 37, loss = 0.00203937\n",
      "Iteration 38, loss = 0.02076518\n",
      "Iteration 39, loss = 0.01215226\n",
      "Iteration 40, loss = 0.00603434\n",
      "Iteration 41, loss = 0.00298669\n",
      "Iteration 42, loss = 0.00228074\n",
      "Iteration 43, loss = 0.00263737\n",
      "Iteration 44, loss = 0.00472852\n",
      "Iteration 45, loss = 0.00761366\n",
      "Iteration 46, loss = 0.00547913\n",
      "Iteration 47, loss = 0.00304077\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[Voting] ............ (5 of 5) Processing MLPClassifier, total=27.8min\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;SVM&#x27;, SVC(C=5.0)),\n",
       "                             (&#x27;Logistic Regression&#x27;,\n",
       "                              LogisticRegression(max_iter=1000,\n",
       "                                                 multi_class=&#x27;multinomial&#x27;)),\n",
       "                             (&#x27;Gradient Boosting&#x27;,\n",
       "                              GradientBoostingClassifier(learning_rate=0.5,\n",
       "                                                         loss=&#x27;exponential&#x27;,\n",
       "                                                         n_estimators=200)),\n",
       "                             (&#x27;AdaBoost&#x27;,\n",
       "                              AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=5),\n",
       "                                                 learning_rate=0.5,\n",
       "                                                 n_estimators=115)),\n",
       "                             (&#x27;MLPClassifier&#x27;,\n",
       "                              MLPClassifier(hidden_layer_sizes=(600, 10),\n",
       "                                            learning_rate=&#x27;adaptive&#x27;,\n",
       "                                            max_iter=300, verbose=True))],\n",
       "                 verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;SVM&#x27;, SVC(C=5.0)),\n",
       "                             (&#x27;Logistic Regression&#x27;,\n",
       "                              LogisticRegression(max_iter=1000,\n",
       "                                                 multi_class=&#x27;multinomial&#x27;)),\n",
       "                             (&#x27;Gradient Boosting&#x27;,\n",
       "                              GradientBoostingClassifier(learning_rate=0.5,\n",
       "                                                         loss=&#x27;exponential&#x27;,\n",
       "                                                         n_estimators=200)),\n",
       "                             (&#x27;AdaBoost&#x27;,\n",
       "                              AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=5),\n",
       "                                                 learning_rate=0.5,\n",
       "                                                 n_estimators=115)),\n",
       "                             (&#x27;MLPClassifier&#x27;,\n",
       "                              MLPClassifier(hidden_layer_sizes=(600, 10),\n",
       "                                            learning_rate=&#x27;adaptive&#x27;,\n",
       "                                            max_iter=300, verbose=True))],\n",
       "                 verbose=True)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>SVM</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=5.0)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>Logistic Regression</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, multi_class=&#x27;multinomial&#x27;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>Gradient Boosting</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(learning_rate=0.5, loss=&#x27;exponential&#x27;,\n",
       "                           n_estimators=200)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>AdaBoost</label></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=5)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=5)</pre></div></div></div></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>MLPClassifier</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(600, 10), learning_rate=&#x27;adaptive&#x27;,\n",
       "              max_iter=300, verbose=True)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('SVM', SVC(C=5.0)),\n",
       "                             ('Logistic Regression',\n",
       "                              LogisticRegression(max_iter=1000,\n",
       "                                                 multi_class='multinomial')),\n",
       "                             ('Gradient Boosting',\n",
       "                              GradientBoostingClassifier(learning_rate=0.5,\n",
       "                                                         loss='exponential',\n",
       "                                                         n_estimators=200)),\n",
       "                             ('AdaBoost',\n",
       "                              AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=5),\n",
       "                                                 learning_rate=0.5,\n",
       "                                                 n_estimators=115)),\n",
       "                             ('MLPClassifier',\n",
       "                              MLPClassifier(hidden_layer_sizes=(600, 10),\n",
       "                                            learning_rate='adaptive',\n",
       "                                            max_iter=300, verbose=True))],\n",
       "                 verbose=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_classifier.fit(dense_count_with_linguistic, dense_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9593816806446308\n"
     ]
    }
   ],
   "source": [
    "text_test_preprocessed = text_test.apply(preprocess_text)\n",
    "test_count_matrix = count_vectorizer.transform(text_test_preprocessed)\n",
    "dense_test_count_matrix = svd.transform(test_count_matrix)\n",
    "dense_test_count_with_linguistic = hstack([dense_test_count_matrix, csr_matrix(test_linguistic_features)])\n",
    "test_labels = test_data['label'].values\n",
    "\n",
    "test_predictions = voting_classifier.predict(dense_test_count_with_linguistic)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f'Test accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96      3420\n",
      "           1       0.95      0.96      0.95      2661\n",
      "\n",
      "    accuracy                           0.96      6081\n",
      "   macro avg       0.96      0.96      0.96      6081\n",
      "weighted avg       0.96      0.96      0.96      6081\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_labels, test_predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['votingclassifier_model_cv.joblib']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "dump(voting_classifier, 'votingclassifier_model_cv.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5834.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy*lengthTestData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
