{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LEGION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LEGION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LEGION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Bidirectional, Dense, Dropout, Conv1D, MaxPooling1D, GlobalAveragePooling1D, BatchNormalization, Activation\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_data = pd.read_csv('train.csv')\n",
    "val_data = pd.read_csv('val.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "lengthTestData = len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine text data for preprocessing\n",
    "text = pd.concat([train_data['text'], val_data['text']], ignore_index=True)\n",
    "text_test = test_data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "linguistic_features = train_data.iloc[:,-9:]\n",
    "val_linguistic_features = val_data.iloc[:,-9:]\n",
    "test_linguistic_features = test_data.iloc[:,-9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words_per_Sentence</th>\n",
       "      <th>Percentage_Questions</th>\n",
       "      <th>Percentage_First_Person_Singular</th>\n",
       "      <th>Percentage_Second_Person</th>\n",
       "      <th>Percentage_Third_Person</th>\n",
       "      <th>Percentage_Negation</th>\n",
       "      <th>Percentage_Exclusive</th>\n",
       "      <th>Percentage_Causation</th>\n",
       "      <th>Percentage_Sense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.930502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.702703</td>\n",
       "      <td>5.405405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.735562</td>\n",
       "      <td>0.607903</td>\n",
       "      <td>1.519757</td>\n",
       "      <td>1.519757</td>\n",
       "      <td>4.863222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.416667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.191781</td>\n",
       "      <td>0.547945</td>\n",
       "      <td>0.821918</td>\n",
       "      <td>1.095890</td>\n",
       "      <td>6.027397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.254545</td>\n",
       "      <td>3.636364</td>\n",
       "      <td>0.085543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.790419</td>\n",
       "      <td>1.197605</td>\n",
       "      <td>1.710864</td>\n",
       "      <td>2.138580</td>\n",
       "      <td>3.079555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>3.181818</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>1.636364</td>\n",
       "      <td>2.181818</td>\n",
       "      <td>6.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48634</th>\n",
       "      <td>17.466667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.908397</td>\n",
       "      <td>0.763359</td>\n",
       "      <td>3.053435</td>\n",
       "      <td>0.381679</td>\n",
       "      <td>1.145038</td>\n",
       "      <td>0.381679</td>\n",
       "      <td>3.053435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48635</th>\n",
       "      <td>23.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.723404</td>\n",
       "      <td>2.127660</td>\n",
       "      <td>1.063830</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>1.063830</td>\n",
       "      <td>1.063830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48636</th>\n",
       "      <td>25.785714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.908587</td>\n",
       "      <td>0.415512</td>\n",
       "      <td>1.800554</td>\n",
       "      <td>0.969529</td>\n",
       "      <td>0.831025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48637</th>\n",
       "      <td>26.809524</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>0.532860</td>\n",
       "      <td>0.177620</td>\n",
       "      <td>8.703375</td>\n",
       "      <td>0.355240</td>\n",
       "      <td>1.598579</td>\n",
       "      <td>1.598579</td>\n",
       "      <td>2.131439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48638</th>\n",
       "      <td>29.176471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.814516</td>\n",
       "      <td>0.403226</td>\n",
       "      <td>4.032258</td>\n",
       "      <td>1.008065</td>\n",
       "      <td>2.217742</td>\n",
       "      <td>1.209677</td>\n",
       "      <td>2.016129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48639 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Words_per_Sentence  Percentage_Questions  \\\n",
       "0               28.777778              0.000000   \n",
       "1               32.900000              0.000000   \n",
       "2               30.416667              0.000000   \n",
       "3               21.254545              3.636364   \n",
       "4               24.444444              0.000000   \n",
       "...                   ...                   ...   \n",
       "48634           17.466667              0.000000   \n",
       "48635           23.500000              0.000000   \n",
       "48636           25.785714              0.000000   \n",
       "48637           26.809524              4.761905   \n",
       "48638           29.176471              0.000000   \n",
       "\n",
       "       Percentage_First_Person_Singular  Percentage_Second_Person  \\\n",
       "0                              0.000000                  0.000000   \n",
       "1                              0.000000                  0.000000   \n",
       "2                              0.000000                  0.000000   \n",
       "3                              0.085543                  0.000000   \n",
       "4                              0.000000                  0.090909   \n",
       "...                                 ...                       ...   \n",
       "48634                          1.908397                  0.763359   \n",
       "48635                          3.723404                  2.127660   \n",
       "48636                          0.000000                  0.000000   \n",
       "48637                          0.532860                  0.177620   \n",
       "48638                          1.814516                  0.403226   \n",
       "\n",
       "       Percentage_Third_Person  Percentage_Negation  Percentage_Exclusive  \\\n",
       "0                     1.930502             0.000000              0.000000   \n",
       "1                     2.735562             0.607903              1.519757   \n",
       "2                     2.191781             0.547945              0.821918   \n",
       "3                     4.790419             1.197605              1.710864   \n",
       "4                     3.181818             0.545455              1.636364   \n",
       "...                        ...                  ...                   ...   \n",
       "48634                 3.053435             0.381679              1.145038   \n",
       "48635                 1.063830             0.531915              0.531915   \n",
       "48636                 2.908587             0.415512              1.800554   \n",
       "48637                 8.703375             0.355240              1.598579   \n",
       "48638                 4.032258             1.008065              2.217742   \n",
       "\n",
       "       Percentage_Causation  Percentage_Sense  \n",
       "0                  2.702703          5.405405  \n",
       "1                  1.519757          4.863222  \n",
       "2                  1.095890          6.027397  \n",
       "3                  2.138580          3.079555  \n",
       "4                  2.181818          6.181818  \n",
       "...                     ...               ...  \n",
       "48634              0.381679          3.053435  \n",
       "48635              1.063830          1.063830  \n",
       "48636              0.969529          0.831025  \n",
       "48637              1.598579          2.131439  \n",
       "48638              1.209677          2.016129  \n",
       "\n",
       "[48639 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linguistic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words_per_Sentence</th>\n",
       "      <th>Percentage_Questions</th>\n",
       "      <th>Percentage_First_Person_Singular</th>\n",
       "      <th>Percentage_Second_Person</th>\n",
       "      <th>Percentage_Third_Person</th>\n",
       "      <th>Percentage_Negation</th>\n",
       "      <th>Percentage_Exclusive</th>\n",
       "      <th>Percentage_Causation</th>\n",
       "      <th>Percentage_Sense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.866667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.078522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.461894</td>\n",
       "      <td>2.078522</td>\n",
       "      <td>1.154734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.787234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.851064</td>\n",
       "      <td>1.063830</td>\n",
       "      <td>1.595745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.709677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.261097</td>\n",
       "      <td>1.958225</td>\n",
       "      <td>1.174935</td>\n",
       "      <td>2.349869</td>\n",
       "      <td>1.044386</td>\n",
       "      <td>6.527415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.380368</td>\n",
       "      <td>0.153374</td>\n",
       "      <td>4.447853</td>\n",
       "      <td>0.613497</td>\n",
       "      <td>1.380368</td>\n",
       "      <td>1.533742</td>\n",
       "      <td>5.674847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6075</th>\n",
       "      <td>56.166667</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>0.296736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.154303</td>\n",
       "      <td>1.186944</td>\n",
       "      <td>1.186944</td>\n",
       "      <td>0.593472</td>\n",
       "      <td>5.341246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6076</th>\n",
       "      <td>22.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.183406</td>\n",
       "      <td>0.436681</td>\n",
       "      <td>0.873362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.240175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6077</th>\n",
       "      <td>18.095238</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>1.842105</td>\n",
       "      <td>3.684211</td>\n",
       "      <td>5.263158</td>\n",
       "      <td>1.052632</td>\n",
       "      <td>2.105263</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>5.789474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6078</th>\n",
       "      <td>16.611354</td>\n",
       "      <td>10.043668</td>\n",
       "      <td>1.156677</td>\n",
       "      <td>2.576236</td>\n",
       "      <td>2.970557</td>\n",
       "      <td>1.445846</td>\n",
       "      <td>2.602524</td>\n",
       "      <td>1.393270</td>\n",
       "      <td>2.628812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6079</th>\n",
       "      <td>28.111111</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>0.988142</td>\n",
       "      <td>0.592885</td>\n",
       "      <td>4.743083</td>\n",
       "      <td>1.185771</td>\n",
       "      <td>1.185771</td>\n",
       "      <td>1.383399</td>\n",
       "      <td>4.743083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6080 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Words_per_Sentence  Percentage_Questions  \\\n",
       "0              28.866667              0.000000   \n",
       "1              15.666667              0.000000   \n",
       "2              24.709677              0.000000   \n",
       "3              25.076923              0.000000   \n",
       "4              25.000000              0.000000   \n",
       "...                  ...                   ...   \n",
       "6075           56.166667             16.666667   \n",
       "6076           22.900000              0.000000   \n",
       "6077           18.095238             14.285714   \n",
       "6078           16.611354             10.043668   \n",
       "6079           28.111111             11.111111   \n",
       "\n",
       "      Percentage_First_Person_Singular  Percentage_Second_Person  \\\n",
       "0                             0.000000                  0.000000   \n",
       "1                             4.787234                  0.000000   \n",
       "2                             0.000000                  0.261097   \n",
       "3                             1.380368                  0.153374   \n",
       "4                             0.000000                  0.000000   \n",
       "...                                ...                       ...   \n",
       "6075                          0.296736                  0.000000   \n",
       "6076                          0.000000                  0.000000   \n",
       "6077                          1.842105                  3.684211   \n",
       "6078                          1.156677                  2.576236   \n",
       "6079                          0.988142                  0.592885   \n",
       "\n",
       "      Percentage_Third_Person  Percentage_Negation  Percentage_Exclusive  \\\n",
       "0                    2.078522             0.000000              0.461894   \n",
       "1                    5.851064             1.063830              1.595745   \n",
       "2                    1.958225             1.174935              2.349869   \n",
       "3                    4.447853             0.613497              1.380368   \n",
       "4                    2.000000             0.000000              1.000000   \n",
       "...                       ...                  ...                   ...   \n",
       "6075                 4.154303             1.186944              1.186944   \n",
       "6076                 2.183406             0.436681              0.873362   \n",
       "6077                 5.263158             1.052632              2.105263   \n",
       "6078                 2.970557             1.445846              2.602524   \n",
       "6079                 4.743083             1.185771              1.185771   \n",
       "\n",
       "      Percentage_Causation  Percentage_Sense  \n",
       "0                 2.078522          1.154734  \n",
       "1                 0.000000          0.000000  \n",
       "2                 1.044386          6.527415  \n",
       "3                 1.533742          5.674847  \n",
       "4                 2.000000          2.000000  \n",
       "...                    ...               ...  \n",
       "6075              0.593472          5.341246  \n",
       "6076              0.000000          5.240175  \n",
       "6077              1.315789          5.789474  \n",
       "6078              1.393270          2.628812  \n",
       "6079              1.383399          4.743083  \n",
       "\n",
       "[6080 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_linguistic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words_per_Sentence</th>\n",
       "      <th>Percentage_Questions</th>\n",
       "      <th>Percentage_First_Person_Singular</th>\n",
       "      <th>Percentage_Second_Person</th>\n",
       "      <th>Percentage_Third_Person</th>\n",
       "      <th>Percentage_Negation</th>\n",
       "      <th>Percentage_Exclusive</th>\n",
       "      <th>Percentage_Causation</th>\n",
       "      <th>Percentage_Sense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.265957</td>\n",
       "      <td>2.393617</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.595745</td>\n",
       "      <td>2.393617</td>\n",
       "      <td>7.446809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>2.142857</td>\n",
       "      <td>2.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.369863</td>\n",
       "      <td>1.369863</td>\n",
       "      <td>1.369863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.219178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.466667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.404531</td>\n",
       "      <td>0.242718</td>\n",
       "      <td>3.559871</td>\n",
       "      <td>0.485437</td>\n",
       "      <td>0.566343</td>\n",
       "      <td>1.779935</td>\n",
       "      <td>3.478964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.278689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>3.278689</td>\n",
       "      <td>6.557377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6076</th>\n",
       "      <td>26.708333</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>0.468019</td>\n",
       "      <td>0.624025</td>\n",
       "      <td>4.680187</td>\n",
       "      <td>0.936037</td>\n",
       "      <td>2.184087</td>\n",
       "      <td>1.560062</td>\n",
       "      <td>1.872075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6077</th>\n",
       "      <td>32.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.834862</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.305810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.587156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6078</th>\n",
       "      <td>25.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.427083</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>2.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6079</th>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>2.380952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6080</th>\n",
       "      <td>17.125000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.729927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.029197</td>\n",
       "      <td>0.729927</td>\n",
       "      <td>5.109489</td>\n",
       "      <td>2.919708</td>\n",
       "      <td>5.109489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6081 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Words_per_Sentence  Percentage_Questions  \\\n",
       "0              25.066667              0.000000   \n",
       "1              23.333333              0.000000   \n",
       "2              24.333333              0.000000   \n",
       "3              27.466667              0.000000   \n",
       "4              40.666667              0.000000   \n",
       "...                  ...                   ...   \n",
       "6076           26.708333              8.333333   \n",
       "6077           32.700000              0.000000   \n",
       "6078           25.600000              0.000000   \n",
       "6079           63.000000              0.000000   \n",
       "6080           17.125000             25.000000   \n",
       "\n",
       "      Percentage_First_Person_Singular  Percentage_Second_Person  \\\n",
       "0                             0.000000                  0.265957   \n",
       "1                             0.000000                  0.000000   \n",
       "2                             0.000000                  0.000000   \n",
       "3                             0.404531                  0.242718   \n",
       "4                             0.000000                  0.000000   \n",
       "...                                ...                       ...   \n",
       "6076                          0.468019                  0.624025   \n",
       "6077                          0.000000                  0.000000   \n",
       "6078                          0.000000                  0.000000   \n",
       "6079                          0.000000                  0.000000   \n",
       "6080                          0.729927                  0.000000   \n",
       "\n",
       "      Percentage_Third_Person  Percentage_Negation  Percentage_Exclusive  \\\n",
       "0                    2.393617             0.000000              1.595745   \n",
       "1                    2.857143             2.857143              2.857143   \n",
       "2                    1.369863             1.369863              1.369863   \n",
       "3                    3.559871             0.485437              0.566343   \n",
       "4                    3.278689             0.000000              0.819672   \n",
       "...                       ...                  ...                   ...   \n",
       "6076                 4.680187             0.936037              2.184087   \n",
       "6077                 1.834862             0.000000              0.305810   \n",
       "6078                 4.427083             0.520833              0.781250   \n",
       "6079                 5.555556             0.000000              2.777778   \n",
       "6080                 8.029197             0.729927              5.109489   \n",
       "\n",
       "      Percentage_Causation  Percentage_Sense  \n",
       "0                 2.393617          7.446809  \n",
       "1                 2.142857          2.857143  \n",
       "2                 0.000000          8.219178  \n",
       "3                 1.779935          3.478964  \n",
       "4                 3.278689          6.557377  \n",
       "...                    ...               ...  \n",
       "6076              1.560062          1.872075  \n",
       "6077              0.000000          4.587156  \n",
       "6078              3.125000          2.083333  \n",
       "6079              0.793651          2.380952  \n",
       "6080              2.919708          5.109489  \n",
       "\n",
       "[6081 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_linguistic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenization and lowercase\n",
    "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]  # Remove stopwords and lemmatize\n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessed = text.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        beirut ( reuters ) - iran military chief met s...\n",
      "1        hanoi ( reuters ) - top u.s. envoy began two-d...\n",
      "2        ( reuters ) - four u.s. senator asked senate j...\n",
      "3        first read morning briefing meet press nbc pol...\n",
      "4        cairo ( reuters ) - six month egypt election ,...\n",
      "                               ...                        \n",
      "54714    lack oversight prof donald trump totally unfit...\n",
      "54715    tucker carlson responded espn anchor calling p...\n",
      "54716    getting something nothing rage president profe...\n",
      "54717    black emanuelle fixed 1976. attila speaking eu...\n",
      "54718    chaos broke legal american illegal alien clash...\n",
      "Name: text, Length: 54719, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        beirut (reuters) - iran s military chief met w...\n",
      "1        hanoi (reuters) - a top u.s. envoy began a two...\n",
      "2        (reuters) - four u.s. senators have asked the ...\n",
      "3        first read is a morning briefing from meet the...\n",
      "4        cairo (reuters) - six months before egypt s el...\n",
      "                               ...                        \n",
      "54714    this lack of oversight proves that donald trum...\n",
      "54715    tucker carlson responded to an espn anchor cal...\n",
      "54716    because getting something for nothing is all t...\n",
      "54717    black emanuelle fixed all that in 1976. attila...\n",
      "54718    chaos broke out after legal americans and ille...\n",
      "Name: text, Length: 54719, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF vectorizer without specifying max_features\n",
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the preprocessed text data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169079\n"
     ]
    }
   ],
   "source": [
    "# Get the number of unique tokens\n",
    "num_unique_tokens = len(tfidf_vectorizer.get_feature_names_out())\n",
    "print(num_unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize TF-IDF vectorizer with the determined max_features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=num_unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3498\n"
     ]
    }
   ],
   "source": [
    "#Fit and transform the text data again with the updated max_features\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(text_preprocessed)\n",
    "\n",
    "# Convert the TF-IDF matrix to a CSR (Compressed Sparse Row) matrix for efficient row-wise operations\n",
    "csr_tfidf_matrix = csr_matrix(tfidf_matrix)\n",
    "\n",
    "# Find the row index with the maximum number of filled values\n",
    "max_features_row_index = csr_tfidf_matrix.getnnz(axis=1).argmax()\n",
    "\n",
    "# Get the number of features in the document with the most filled values\n",
    "max_features = csr_tfidf_matrix[max_features_row_index].count_nonzero()\n",
    "\n",
    "print(max_features)\n",
    "\n",
    "svd = TruncatedSVD(n_components=int(max_features*0.3))\n",
    "tfidf_matrix = svd.fit_transform(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_tfidf_matrix = tfidf_matrix[:len(train_data)]\n",
    "dense_tfidf_with_linguistic = hstack([dense_tfidf_matrix, csr_matrix(linguistic_features)])\n",
    "\n",
    "dense_val_tfidf_matrix = tfidf_matrix[len(train_data):len(train_data) + len(val_data)]\n",
    "dense_val_tfidf_with_linguistic = hstack([dense_val_tfidf_matrix, csr_matrix(val_linguistic_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the CNN + Bi-LSTM model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=256, kernel_size=2, activation='relu', input_shape=(dense_tfidf_with_linguistic.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=64, kernel_size=4, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(16))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Define the Bi-LSTM model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(tfidf_matrix.shape[1],1)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1520/1520 [==============================] - 488s 316ms/step - loss: 0.4190 - accuracy: 0.8085 - val_loss: 0.2904 - val_accuracy: 0.8799 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "1520/1520 [==============================] - 421s 277ms/step - loss: 0.3119 - accuracy: 0.8703 - val_loss: 0.2756 - val_accuracy: 0.8895 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "1520/1520 [==============================] - 428s 282ms/step - loss: 0.2940 - accuracy: 0.8774 - val_loss: 0.2596 - val_accuracy: 0.8913 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "1520/1520 [==============================] - 631s 415ms/step - loss: 0.2754 - accuracy: 0.8872 - val_loss: 0.2525 - val_accuracy: 0.8938 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "1520/1520 [==============================] - 628s 413ms/step - loss: 0.2678 - accuracy: 0.8894 - val_loss: 0.2659 - val_accuracy: 0.8910 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "1520/1520 [==============================] - 608s 400ms/step - loss: 0.2599 - accuracy: 0.8929 - val_loss: 0.2479 - val_accuracy: 0.8992 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "1520/1520 [==============================] - 607s 399ms/step - loss: 0.2530 - accuracy: 0.8959 - val_loss: 0.2400 - val_accuracy: 0.9036 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "1520/1520 [==============================] - 600s 395ms/step - loss: 0.2461 - accuracy: 0.8980 - val_loss: 0.2421 - val_accuracy: 0.8984 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "1520/1520 [==============================] - 599s 394ms/step - loss: 0.2402 - accuracy: 0.9018 - val_loss: 0.2360 - val_accuracy: 0.9030 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "1520/1520 [==============================] - 603s 397ms/step - loss: 0.2352 - accuracy: 0.9041 - val_loss: 0.2354 - val_accuracy: 0.9074 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "1520/1520 [==============================] - 601s 396ms/step - loss: 0.2321 - accuracy: 0.9067 - val_loss: 0.2318 - val_accuracy: 0.9066 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "1520/1520 [==============================] - 605s 398ms/step - loss: 0.2218 - accuracy: 0.9109 - val_loss: 0.2274 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "1520/1520 [==============================] - 605s 398ms/step - loss: 0.2200 - accuracy: 0.9117 - val_loss: 0.2212 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "1520/1520 [==============================] - 605s 398ms/step - loss: 0.2143 - accuracy: 0.9149 - val_loss: 0.2324 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "1520/1520 [==============================] - 609s 400ms/step - loss: 0.2084 - accuracy: 0.9168 - val_loss: 0.2342 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "1520/1520 [==============================] - 602s 396ms/step - loss: 0.2049 - accuracy: 0.9186 - val_loss: 0.2210 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "1520/1520 [==============================] - 601s 396ms/step - loss: 0.1982 - accuracy: 0.9211 - val_loss: 0.2210 - val_accuracy: 0.9104 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "1520/1520 [==============================] - 600s 395ms/step - loss: 0.1949 - accuracy: 0.9234 - val_loss: 0.2164 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "1520/1520 [==============================] - 424s 279ms/step - loss: 0.1899 - accuracy: 0.9250 - val_loss: 0.2180 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "1520/1520 [==============================] - 384s 253ms/step - loss: 0.1856 - accuracy: 0.9260 - val_loss: 0.2605 - val_accuracy: 0.8998 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "1520/1520 [==============================] - 386s 254ms/step - loss: 0.1827 - accuracy: 0.9285 - val_loss: 0.2264 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "1520/1520 [==============================] - 385s 253ms/step - loss: 0.1762 - accuracy: 0.9300 - val_loss: 0.2249 - val_accuracy: 0.9135 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "1520/1520 [==============================] - ETA: 0s - loss: 0.1696 - accuracy: 0.9339\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "1520/1520 [==============================] - 384s 253ms/step - loss: 0.1696 - accuracy: 0.9339 - val_loss: 0.2415 - val_accuracy: 0.9064 - lr: 0.0010\n",
      "Epoch 23: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2e7207b59a0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming train_data['label'] and val_data['label'] are Pandas Series, convert them to arrays\n",
    "train_labels = train_data['label'].values\n",
    "val_labels = val_data['label'].values\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1)\n",
    "\n",
    "# Reshape the input data\n",
    "dense_tfidf_with_linguistic = dense_tfidf_with_linguistic.toarray()  # Convert to dense array\n",
    "dense_val_tfidf_with_linguistic = dense_val_tfidf_with_linguistic.toarray()  # Convert to dense array\n",
    "\n",
    "# Fit the model using NumPy arrays with callbacks\n",
    "model.fit(dense_tfidf_with_linguistic, train_labels, \n",
    "          epochs=100, batch_size=32, \n",
    "          validation_data=(dense_val_tfidf_with_linguistic, val_labels),\n",
    "          callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191/191 [==============================] - 22s 103ms/step - loss: 0.2277 - accuracy: 0.9076\n",
      "Test accuracy: 0.9075809717178345\n"
     ]
    }
   ],
   "source": [
    "text_test_preprocessed = text_test.apply(preprocess_text)\n",
    "test_tfidf_matrix = tfidf_vectorizer.transform(text_test_preprocessed)\n",
    "dense_test_tfidf_matrix = svd.transform(test_tfidf_matrix)\n",
    "dense_test_tfidf_with_linguistic = hstack([dense_test_tfidf_matrix, csr_matrix(test_linguistic_features)])\n",
    "test_labels = test_data['label'].values\n",
    "\n",
    "dense_test_tfidf_with_linguistic = dense_test_tfidf_with_linguistic.toarray()  # Convert to dense array\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "test_loss, test_accuracy = model.evaluate(dense_test_tfidf_with_linguistic, test_labels)\n",
    "print(f'Test accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('bi_lstm_tfidf.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5518.999889016151"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy * lengthTestData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
