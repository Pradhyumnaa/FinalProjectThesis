{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LEGION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LEGION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LEGION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_data = pd.read_csv('train.csv')\n",
    "val_data = pd.read_csv('val.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "lengthTestData = len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine text data for preprocessing\n",
    "text = pd.concat([train_data['text'], val_data['text']], ignore_index=True)\n",
    "text_test = test_data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "linguistic_features = pd.concat([train_data.iloc[:,-9:], val_data.iloc[:,-9:]], ignore_index=True)\n",
    "test_linguistic_features = test_data.iloc[:,-9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words_per_Sentence</th>\n",
       "      <th>Percentage_Questions</th>\n",
       "      <th>Percentage_First_Person_Singular</th>\n",
       "      <th>Percentage_Second_Person</th>\n",
       "      <th>Percentage_Third_Person</th>\n",
       "      <th>Percentage_Negation</th>\n",
       "      <th>Percentage_Exclusive</th>\n",
       "      <th>Percentage_Causation</th>\n",
       "      <th>Percentage_Sense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.930502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.702703</td>\n",
       "      <td>5.405405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.735562</td>\n",
       "      <td>0.607903</td>\n",
       "      <td>1.519757</td>\n",
       "      <td>1.519757</td>\n",
       "      <td>4.863222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.416667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.191781</td>\n",
       "      <td>0.547945</td>\n",
       "      <td>0.821918</td>\n",
       "      <td>1.095890</td>\n",
       "      <td>6.027397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.254545</td>\n",
       "      <td>3.636364</td>\n",
       "      <td>0.085543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.790419</td>\n",
       "      <td>1.197605</td>\n",
       "      <td>1.710864</td>\n",
       "      <td>2.138580</td>\n",
       "      <td>3.079555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>3.181818</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>1.636364</td>\n",
       "      <td>2.181818</td>\n",
       "      <td>6.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54714</th>\n",
       "      <td>56.166667</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>0.296736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.154303</td>\n",
       "      <td>1.186944</td>\n",
       "      <td>1.186944</td>\n",
       "      <td>0.593472</td>\n",
       "      <td>5.341246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54715</th>\n",
       "      <td>22.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.183406</td>\n",
       "      <td>0.436681</td>\n",
       "      <td>0.873362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.240175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54716</th>\n",
       "      <td>18.095238</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>1.842105</td>\n",
       "      <td>3.684211</td>\n",
       "      <td>5.263158</td>\n",
       "      <td>1.052632</td>\n",
       "      <td>2.105263</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>5.789474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54717</th>\n",
       "      <td>16.611354</td>\n",
       "      <td>10.043668</td>\n",
       "      <td>1.156677</td>\n",
       "      <td>2.576236</td>\n",
       "      <td>2.970557</td>\n",
       "      <td>1.445846</td>\n",
       "      <td>2.602524</td>\n",
       "      <td>1.393270</td>\n",
       "      <td>2.628812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54718</th>\n",
       "      <td>28.111111</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>0.988142</td>\n",
       "      <td>0.592885</td>\n",
       "      <td>4.743083</td>\n",
       "      <td>1.185771</td>\n",
       "      <td>1.185771</td>\n",
       "      <td>1.383399</td>\n",
       "      <td>4.743083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54719 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Words_per_Sentence  Percentage_Questions  \\\n",
       "0               28.777778              0.000000   \n",
       "1               32.900000              0.000000   \n",
       "2               30.416667              0.000000   \n",
       "3               21.254545              3.636364   \n",
       "4               24.444444              0.000000   \n",
       "...                   ...                   ...   \n",
       "54714           56.166667             16.666667   \n",
       "54715           22.900000              0.000000   \n",
       "54716           18.095238             14.285714   \n",
       "54717           16.611354             10.043668   \n",
       "54718           28.111111             11.111111   \n",
       "\n",
       "       Percentage_First_Person_Singular  Percentage_Second_Person  \\\n",
       "0                              0.000000                  0.000000   \n",
       "1                              0.000000                  0.000000   \n",
       "2                              0.000000                  0.000000   \n",
       "3                              0.085543                  0.000000   \n",
       "4                              0.000000                  0.090909   \n",
       "...                                 ...                       ...   \n",
       "54714                          0.296736                  0.000000   \n",
       "54715                          0.000000                  0.000000   \n",
       "54716                          1.842105                  3.684211   \n",
       "54717                          1.156677                  2.576236   \n",
       "54718                          0.988142                  0.592885   \n",
       "\n",
       "       Percentage_Third_Person  Percentage_Negation  Percentage_Exclusive  \\\n",
       "0                     1.930502             0.000000              0.000000   \n",
       "1                     2.735562             0.607903              1.519757   \n",
       "2                     2.191781             0.547945              0.821918   \n",
       "3                     4.790419             1.197605              1.710864   \n",
       "4                     3.181818             0.545455              1.636364   \n",
       "...                        ...                  ...                   ...   \n",
       "54714                 4.154303             1.186944              1.186944   \n",
       "54715                 2.183406             0.436681              0.873362   \n",
       "54716                 5.263158             1.052632              2.105263   \n",
       "54717                 2.970557             1.445846              2.602524   \n",
       "54718                 4.743083             1.185771              1.185771   \n",
       "\n",
       "       Percentage_Causation  Percentage_Sense  \n",
       "0                  2.702703          5.405405  \n",
       "1                  1.519757          4.863222  \n",
       "2                  1.095890          6.027397  \n",
       "3                  2.138580          3.079555  \n",
       "4                  2.181818          6.181818  \n",
       "...                     ...               ...  \n",
       "54714              0.593472          5.341246  \n",
       "54715              0.000000          5.240175  \n",
       "54716              1.315789          5.789474  \n",
       "54717              1.393270          2.628812  \n",
       "54718              1.383399          4.743083  \n",
       "\n",
       "[54719 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linguistic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words_per_Sentence</th>\n",
       "      <th>Percentage_Questions</th>\n",
       "      <th>Percentage_First_Person_Singular</th>\n",
       "      <th>Percentage_Second_Person</th>\n",
       "      <th>Percentage_Third_Person</th>\n",
       "      <th>Percentage_Negation</th>\n",
       "      <th>Percentage_Exclusive</th>\n",
       "      <th>Percentage_Causation</th>\n",
       "      <th>Percentage_Sense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.265957</td>\n",
       "      <td>2.393617</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.595745</td>\n",
       "      <td>2.393617</td>\n",
       "      <td>7.446809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>2.142857</td>\n",
       "      <td>2.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.369863</td>\n",
       "      <td>1.369863</td>\n",
       "      <td>1.369863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.219178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.466667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.404531</td>\n",
       "      <td>0.242718</td>\n",
       "      <td>3.559871</td>\n",
       "      <td>0.485437</td>\n",
       "      <td>0.566343</td>\n",
       "      <td>1.779935</td>\n",
       "      <td>3.478964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.278689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>3.278689</td>\n",
       "      <td>6.557377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6076</th>\n",
       "      <td>26.708333</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>0.468019</td>\n",
       "      <td>0.624025</td>\n",
       "      <td>4.680187</td>\n",
       "      <td>0.936037</td>\n",
       "      <td>2.184087</td>\n",
       "      <td>1.560062</td>\n",
       "      <td>1.872075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6077</th>\n",
       "      <td>32.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.834862</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.305810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.587156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6078</th>\n",
       "      <td>25.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.427083</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>2.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6079</th>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>2.380952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6080</th>\n",
       "      <td>17.125000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.729927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.029197</td>\n",
       "      <td>0.729927</td>\n",
       "      <td>5.109489</td>\n",
       "      <td>2.919708</td>\n",
       "      <td>5.109489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6081 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Words_per_Sentence  Percentage_Questions  \\\n",
       "0              25.066667              0.000000   \n",
       "1              23.333333              0.000000   \n",
       "2              24.333333              0.000000   \n",
       "3              27.466667              0.000000   \n",
       "4              40.666667              0.000000   \n",
       "...                  ...                   ...   \n",
       "6076           26.708333              8.333333   \n",
       "6077           32.700000              0.000000   \n",
       "6078           25.600000              0.000000   \n",
       "6079           63.000000              0.000000   \n",
       "6080           17.125000             25.000000   \n",
       "\n",
       "      Percentage_First_Person_Singular  Percentage_Second_Person  \\\n",
       "0                             0.000000                  0.265957   \n",
       "1                             0.000000                  0.000000   \n",
       "2                             0.000000                  0.000000   \n",
       "3                             0.404531                  0.242718   \n",
       "4                             0.000000                  0.000000   \n",
       "...                                ...                       ...   \n",
       "6076                          0.468019                  0.624025   \n",
       "6077                          0.000000                  0.000000   \n",
       "6078                          0.000000                  0.000000   \n",
       "6079                          0.000000                  0.000000   \n",
       "6080                          0.729927                  0.000000   \n",
       "\n",
       "      Percentage_Third_Person  Percentage_Negation  Percentage_Exclusive  \\\n",
       "0                    2.393617             0.000000              1.595745   \n",
       "1                    2.857143             2.857143              2.857143   \n",
       "2                    1.369863             1.369863              1.369863   \n",
       "3                    3.559871             0.485437              0.566343   \n",
       "4                    3.278689             0.000000              0.819672   \n",
       "...                       ...                  ...                   ...   \n",
       "6076                 4.680187             0.936037              2.184087   \n",
       "6077                 1.834862             0.000000              0.305810   \n",
       "6078                 4.427083             0.520833              0.781250   \n",
       "6079                 5.555556             0.000000              2.777778   \n",
       "6080                 8.029197             0.729927              5.109489   \n",
       "\n",
       "      Percentage_Causation  Percentage_Sense  \n",
       "0                 2.393617          7.446809  \n",
       "1                 2.142857          2.857143  \n",
       "2                 0.000000          8.219178  \n",
       "3                 1.779935          3.478964  \n",
       "4                 3.278689          6.557377  \n",
       "...                    ...               ...  \n",
       "6076              1.560062          1.872075  \n",
       "6077              0.000000          4.587156  \n",
       "6078              3.125000          2.083333  \n",
       "6079              0.793651          2.380952  \n",
       "6080              2.919708          5.109489  \n",
       "\n",
       "[6081 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_linguistic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenization and lowercase\n",
    "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]  # Remove stopwords and lemmatize\n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessed = text.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        beirut ( reuters ) - iran military chief met s...\n",
      "1        hanoi ( reuters ) - top u.s. envoy began two-d...\n",
      "2        ( reuters ) - four u.s. senator asked senate j...\n",
      "3        first read morning briefing meet press nbc pol...\n",
      "4        cairo ( reuters ) - six month egypt election ,...\n",
      "                               ...                        \n",
      "54714    lack oversight prof donald trump totally unfit...\n",
      "54715    tucker carlson responded espn anchor calling p...\n",
      "54716    getting something nothing rage president profe...\n",
      "54717    black emanuelle fixed 1976. attila speaking eu...\n",
      "54718    chaos broke legal american illegal alien clash...\n",
      "Name: text, Length: 54719, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        beirut (reuters) - iran s military chief met w...\n",
      "1        hanoi (reuters) - a top u.s. envoy began a two...\n",
      "2        (reuters) - four u.s. senators have asked the ...\n",
      "3        first read is a morning briefing from meet the...\n",
      "4        cairo (reuters) - six months before egypt s el...\n",
      "                               ...                        \n",
      "54714    this lack of oversight proves that donald trum...\n",
      "54715    tucker carlson responded to an espn anchor cal...\n",
      "54716    because getting something for nothing is all t...\n",
      "54717    black emanuelle fixed all that in 1976. attila...\n",
      "54718    chaos broke out after legal americans and ille...\n",
      "Name: text, Length: 54719, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF vectorizer without specifying max_features\n",
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the preprocessed text data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169079\n"
     ]
    }
   ],
   "source": [
    "# Get the number of unique tokens\n",
    "num_unique_tokens = len(tfidf_vectorizer.get_feature_names_out())\n",
    "print(num_unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize TF-IDF vectorizer with the determined max_features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=num_unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit and transform the text data again with the updated max_features\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(text_preprocessed)\n",
    "\n",
    "# Convert the TF-IDF matrix to a CSR (Compressed Sparse Row) matrix for efficient row-wise operations\n",
    "csr_tfidf_matrix = csr_matrix(tfidf_matrix)\n",
    "\n",
    "# Find the row index with the maximum number of filled values\n",
    "max_features_row_index = csr_tfidf_matrix.getnnz(axis=1).argmax()\n",
    "\n",
    "# Get the number of features in the document with the most filled values\n",
    "max_features = csr_tfidf_matrix[max_features_row_index].count_nonzero()\n",
    "\n",
    "svd = TruncatedSVD(n_components=int(max_features*0.3))\n",
    "tfidf_matrix = svd.fit_transform(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dense_tfidf_matrix = tfidf_matrix[:len(train_data)]\n",
    "#dense_val_tfidf_matrix = tfidf_matrix[len(train_data):len(train_data) + len(val_data)]\n",
    "\n",
    "# Merging the Validation and Training Data into one for a larger training dataset.\n",
    "#dense_tfidf_matrix = tfidf_matrix[:len(train_data) + len(val_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate linguistic features and TF-IDF matrix horizontally\n",
    "dense_tfidf_with_linguistic = hstack([tfidf_matrix, csr_matrix(linguistic_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert them into Arrays\n",
    "train_labels = train_data['label'].values\n",
    "val_labels = val_data['label'].values\n",
    "\n",
    "dense_labels = np.concatenate((train_data['label'].values, val_data['label'].values), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base estimator (Decision Tree) with max depth 5\n",
    "base_estimator = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "# Create an AdaBoost classifier with custom settings\n",
    "adaboost_model = AdaBoostClassifier(\n",
    "    estimator=base_estimator,  # Using the custom decision tree as the base estimator\n",
    "    n_estimators=115,  # Increasing the number of estimators to 700\n",
    "    learning_rate=0.5,  # Lowering the learning rate to 0.3\n",
    "    algorithm='SAMME.R'  # Using 'SAMME.R' algorithm for real probability estimates\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=5),\n",
       "                   learning_rate=0.5, n_estimators=115)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=5),\n",
       "                   learning_rate=0.5, n_estimators=115)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=5)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=5)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=5),\n",
       "                   learning_rate=0.5, n_estimators=115)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaboost_model.fit(dense_tfidf_with_linguistic, dense_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9307679657950995\n"
     ]
    }
   ],
   "source": [
    "text_test_preprocessed = text_test.apply(preprocess_text)\n",
    "test_tfidf_matrix = tfidf_vectorizer.transform(text_test_preprocessed)\n",
    "dense_test_tfidf_matrix = svd.transform(test_tfidf_matrix)\n",
    "dense_test_tfidf_with_linguistic = hstack([dense_test_tfidf_matrix, csr_matrix(test_linguistic_features)])\n",
    "test_labels = test_data['label'].values\n",
    "\n",
    "test_predictions = adaboost_model.predict(dense_test_tfidf_with_linguistic)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f'Test accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94      3420\n",
      "           1       0.93      0.92      0.92      2661\n",
      "\n",
      "    accuracy                           0.93      6081\n",
      "   macro avg       0.93      0.93      0.93      6081\n",
      "weighted avg       0.93      0.93      0.93      6081\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_labels, test_predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adaboost_tfidf.joblib']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "dump(adaboost_model, 'adaboost_tfidf.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5660.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy*lengthTestData"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
