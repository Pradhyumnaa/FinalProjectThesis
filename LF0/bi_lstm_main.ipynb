{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LEGION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LEGION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LEGION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Bidirectional, Dense, Dropout, Conv1D, MaxPooling1D, GlobalAveragePooling1D, BatchNormalization, Activation\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_data = pd.read_csv('train.csv')\n",
    "val_data = pd.read_csv('val.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "lengthTestData = len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine text data for preprocessing\n",
    "text = pd.concat([train_data['text'], val_data['text']], ignore_index=True)\n",
    "text_test = test_data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenization and lowercase\n",
    "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]  # Remove stopwords and lemmatize\n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessed = text.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        beirut ( reuters ) - iran military chief met s...\n",
      "1        hanoi ( reuters ) - top u.s. envoy began two-d...\n",
      "2        ( reuters ) - four u.s. senator asked senate j...\n",
      "3        first read morning briefing meet press nbc pol...\n",
      "4        cairo ( reuters ) - six month egypt election ,...\n",
      "                               ...                        \n",
      "54714    lack oversight prof donald trump totally unfit...\n",
      "54715    tucker carlson responded espn anchor calling p...\n",
      "54716    getting something nothing rage president profe...\n",
      "54717    black emanuelle fixed 1976. attila speaking eu...\n",
      "54718    chaos broke legal american illegal alien clash...\n",
      "Name: text, Length: 54719, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        beirut (reuters) - iran s military chief met w...\n",
      "1        hanoi (reuters) - a top u.s. envoy began a two...\n",
      "2        (reuters) - four u.s. senators have asked the ...\n",
      "3        first read is a morning briefing from meet the...\n",
      "4        cairo (reuters) - six months before egypt s el...\n",
      "                               ...                        \n",
      "54714    this lack of oversight proves that donald trum...\n",
      "54715    tucker carlson responded to an espn anchor cal...\n",
      "54716    because getting something for nothing is all t...\n",
      "54717    black emanuelle fixed all that in 1976. attila...\n",
      "54718    chaos broke out after legal americans and ille...\n",
      "Name: text, Length: 54719, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF vectorizer without specifying max_features\n",
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the preprocessed text data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169079\n"
     ]
    }
   ],
   "source": [
    "# Get the number of unique tokens\n",
    "num_unique_tokens = len(tfidf_vectorizer.get_feature_names_out())\n",
    "print(num_unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize TF-IDF vectorizer with the determined max_features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=num_unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3498\n"
     ]
    }
   ],
   "source": [
    "#Fit and transform the text data again with the updated max_features\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(text_preprocessed)\n",
    "\n",
    "# Convert the TF-IDF matrix to a CSR (Compressed Sparse Row) matrix for efficient row-wise operations\n",
    "csr_tfidf_matrix = csr_matrix(tfidf_matrix)\n",
    "\n",
    "# Find the row index with the maximum number of filled values\n",
    "max_features_row_index = csr_tfidf_matrix.getnnz(axis=1).argmax()\n",
    "\n",
    "# Get the number of features in the document with the most filled values\n",
    "max_features = csr_tfidf_matrix[max_features_row_index].count_nonzero()\n",
    "\n",
    "print(max_features)\n",
    "\n",
    "svd = TruncatedSVD(n_components=int(max_features*0.3))\n",
    "tfidf_matrix = svd.fit_transform(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the CNN + Bi-LSTM model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=256, kernel_size=2, activation='relu', input_shape=(tfidf_matrix.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=64, kernel_size=4, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(16))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Define the Bi-LSTM model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(tfidf_matrix.shape[1],1)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1520/1520 [==============================] - 626s 406ms/step - loss: 0.4290 - accuracy: 0.8089 - val_loss: 0.3428 - val_accuracy: 0.8592 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "1520/1520 [==============================] - 562s 370ms/step - loss: 0.3458 - accuracy: 0.8570 - val_loss: 0.3189 - val_accuracy: 0.8643 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "1520/1520 [==============================] - 549s 361ms/step - loss: 0.3209 - accuracy: 0.8692 - val_loss: 0.3052 - val_accuracy: 0.8737 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "1520/1520 [==============================] - 590s 388ms/step - loss: 0.3062 - accuracy: 0.8734 - val_loss: 0.2816 - val_accuracy: 0.8827 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "1520/1520 [==============================] - 557s 366ms/step - loss: 0.2969 - accuracy: 0.8792 - val_loss: 0.2973 - val_accuracy: 0.8778 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "1520/1520 [==============================] - 568s 374ms/step - loss: 0.2867 - accuracy: 0.8819 - val_loss: 0.2723 - val_accuracy: 0.8849 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "1520/1520 [==============================] - 580s 381ms/step - loss: 0.2775 - accuracy: 0.8861 - val_loss: 0.2727 - val_accuracy: 0.8865 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "1520/1520 [==============================] - 622s 409ms/step - loss: 0.2796 - accuracy: 0.8867 - val_loss: 0.2626 - val_accuracy: 0.8883 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "1520/1520 [==============================] - 549s 361ms/step - loss: 0.2616 - accuracy: 0.8935 - val_loss: 0.2748 - val_accuracy: 0.8918 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "1520/1520 [==============================] - 541s 356ms/step - loss: 0.2563 - accuracy: 0.8965 - val_loss: 0.2565 - val_accuracy: 0.8941 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "1520/1520 [==============================] - 595s 391ms/step - loss: 0.2470 - accuracy: 0.9005 - val_loss: 0.2515 - val_accuracy: 0.8982 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "1520/1520 [==============================] - 604s 397ms/step - loss: 0.2417 - accuracy: 0.9027 - val_loss: 0.2522 - val_accuracy: 0.8962 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "1520/1520 [==============================] - 583s 384ms/step - loss: 0.2357 - accuracy: 0.9056 - val_loss: 0.2501 - val_accuracy: 0.9015 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "1520/1520 [==============================] - 584s 384ms/step - loss: 0.2295 - accuracy: 0.9084 - val_loss: 0.2433 - val_accuracy: 0.8959 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "1520/1520 [==============================] - 591s 389ms/step - loss: 0.2239 - accuracy: 0.9107 - val_loss: 0.2417 - val_accuracy: 0.8984 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "1520/1520 [==============================] - 542s 356ms/step - loss: 0.2180 - accuracy: 0.9128 - val_loss: 0.2648 - val_accuracy: 0.8959 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "1520/1520 [==============================] - 581s 383ms/step - loss: 0.2125 - accuracy: 0.9151 - val_loss: 0.2414 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "1520/1520 [==============================] - 588s 387ms/step - loss: 0.2067 - accuracy: 0.9180 - val_loss: 0.2558 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "1520/1520 [==============================] - 580s 382ms/step - loss: 0.2009 - accuracy: 0.9198 - val_loss: 0.2513 - val_accuracy: 0.8993 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "1520/1520 [==============================] - 579s 381ms/step - loss: 0.1938 - accuracy: 0.9231 - val_loss: 0.2472 - val_accuracy: 0.8993 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "1520/1520 [==============================] - 588s 387ms/step - loss: 0.1885 - accuracy: 0.9259 - val_loss: 0.2503 - val_accuracy: 0.9046 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "1520/1520 [==============================] - ETA: 0s - loss: 0.1818 - accuracy: 0.9291\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "1520/1520 [==============================] - 567s 373ms/step - loss: 0.1818 - accuracy: 0.9291 - val_loss: 0.2661 - val_accuracy: 0.9002 - lr: 0.0010\n",
      "Epoch 22: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14a72c47fd0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_tfidf_matrix = tfidf_matrix[:len(train_data)]\n",
    "dense_val_tfidf_matrix = tfidf_matrix[len(train_data):len(train_data) + len(val_data)]\n",
    "\n",
    "# Assuming train_data['label'] and val_data['label'] are Pandas Series, convert them to arrays\n",
    "train_labels = train_data['label'].values\n",
    "val_labels = val_data['label'].values\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1)\n",
    "\n",
    "# Fit the model using NumPy arrays with callbacks\n",
    "model.fit(dense_tfidf_matrix, train_labels, \n",
    "          epochs=100, batch_size=32, \n",
    "          validation_data=(dense_val_tfidf_matrix, val_labels),\n",
    "          callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191/191 [==============================] - 23s 112ms/step - loss: 0.2556 - accuracy: 0.9000\n",
      "Test accuracy: 0.9000164270401001\n"
     ]
    }
   ],
   "source": [
    "text_test_preprocessed = text_test.apply(preprocess_text)\n",
    "test_tfidf_matrix = tfidf_vectorizer.transform(text_test_preprocessed)\n",
    "dense_test_tfidf_matrix = svd.transform(test_tfidf_matrix)\n",
    "test_labels = test_data['label'].values\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "test_loss, test_accuracy = model.evaluate(dense_test_tfidf_matrix, test_labels)\n",
    "print(f'Test accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5472.999892830849"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy * lengthTestData"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
