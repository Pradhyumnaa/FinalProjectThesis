{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LEGION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LEGION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LEGION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Bidirectional, Dense, Dropout, Conv1D, MaxPooling1D, GlobalAveragePooling1D, BatchNormalization, Activation\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_data = pd.read_csv('train.csv')\n",
    "val_data = pd.read_csv('val.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "lengthTestData = len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine text data for preprocessing\n",
    "text = pd.concat([train_data['text'], val_data['text']], ignore_index=True)\n",
    "text_test = test_data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenization and lowercase\n",
    "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]  # Remove stopwords and lemmatize\n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessed = text.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        beirut ( reuters ) - iran military chief met s...\n",
      "1        hanoi ( reuters ) - top u.s. envoy began two-d...\n",
      "2        ( reuters ) - four u.s. senator asked senate j...\n",
      "3        first read morning briefing meet press nbc pol...\n",
      "4        cairo ( reuters ) - six month egypt election ,...\n",
      "                               ...                        \n",
      "54714    lack oversight prof donald trump totally unfit...\n",
      "54715    tucker carlson responded espn anchor calling p...\n",
      "54716    getting something nothing rage president profe...\n",
      "54717    black emanuelle fixed 1976. attila speaking eu...\n",
      "54718    chaos broke legal american illegal alien clash...\n",
      "Name: text, Length: 54719, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        beirut (reuters) - iran s military chief met w...\n",
      "1        hanoi (reuters) - a top u.s. envoy began a two...\n",
      "2        (reuters) - four u.s. senators have asked the ...\n",
      "3        first read is a morning briefing from meet the...\n",
      "4        cairo (reuters) - six months before egypt s el...\n",
      "                               ...                        \n",
      "54714    this lack of oversight proves that donald trum...\n",
      "54715    tucker carlson responded to an espn anchor cal...\n",
      "54716    because getting something for nothing is all t...\n",
      "54717    black emanuelle fixed all that in 1976. attila...\n",
      "54718    chaos broke out after legal americans and ille...\n",
      "Name: text, Length: 54719, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Count vectorizer without specifying max_features\n",
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the preprocessed text data\n",
    "count_matrix = count_vectorizer.fit_transform(text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169079\n"
     ]
    }
   ],
   "source": [
    "# Get the number of unique tokens\n",
    "num_unique_tokens = len(count_vectorizer.get_feature_names_out())\n",
    "print(num_unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize Count vectorizer with the determined max_features\n",
    "count_vectorizer = CountVectorizer(max_features=num_unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit and transform the text data again with the updated max_features\n",
    "count_matrix = count_vectorizer.fit_transform(text_preprocessed)\n",
    "\n",
    "# Convert the TF-IDF matrix to a CSR (Compressed Sparse Row) matrix for efficient row-wise operations\n",
    "csr_count_matrix = csr_matrix(count_matrix)\n",
    "\n",
    "# Find the row index with the maximum number of filled values\n",
    "max_features_row_index = csr_count_matrix.getnnz(axis=1).argmax()\n",
    "\n",
    "# Get the number of features in the document with the most filled values\n",
    "max_features = csr_count_matrix[max_features_row_index].count_nonzero()\n",
    "\n",
    "svd = TruncatedSVD(n_components=int(max_features*0.3))\n",
    "count_matrix = svd.fit_transform(count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the CNN + Bi-LSTM model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=256, kernel_size=2, activation='relu', input_shape=(count_matrix.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=64, kernel_size=4, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(16))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Define the Bi-LSTM model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(count_matrix.shape[1],1)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1520/1520 [==============================] - 483s 313ms/step - loss: 0.4056 - accuracy: 0.8217 - val_loss: 0.3573 - val_accuracy: 0.8382 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "1520/1520 [==============================] - 425s 279ms/step - loss: 0.3548 - accuracy: 0.8491 - val_loss: 0.3373 - val_accuracy: 0.8579 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "1520/1520 [==============================] - 406s 267ms/step - loss: 0.3408 - accuracy: 0.8551 - val_loss: 0.3208 - val_accuracy: 0.8633 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "1520/1520 [==============================] - 411s 271ms/step - loss: 0.3289 - accuracy: 0.8617 - val_loss: 0.3148 - val_accuracy: 0.8673 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "1520/1520 [==============================] - 437s 288ms/step - loss: 0.3190 - accuracy: 0.8665 - val_loss: 0.3232 - val_accuracy: 0.8673 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "1520/1520 [==============================] - 429s 282ms/step - loss: 0.3117 - accuracy: 0.8679 - val_loss: 0.3077 - val_accuracy: 0.8671 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "1520/1520 [==============================] - 442s 291ms/step - loss: 0.3032 - accuracy: 0.8731 - val_loss: 0.3010 - val_accuracy: 0.8740 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "1520/1520 [==============================] - 430s 283ms/step - loss: 0.2945 - accuracy: 0.8766 - val_loss: 0.3099 - val_accuracy: 0.8732 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "1520/1520 [==============================] - 430s 283ms/step - loss: 0.2875 - accuracy: 0.8791 - val_loss: 0.3024 - val_accuracy: 0.8747 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "1520/1520 [==============================] - 422s 278ms/step - loss: 0.2805 - accuracy: 0.8831 - val_loss: 0.2941 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "1520/1520 [==============================] - 409s 269ms/step - loss: 0.2715 - accuracy: 0.8879 - val_loss: 0.3053 - val_accuracy: 0.8687 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "1520/1520 [==============================] - 413s 272ms/step - loss: 0.2650 - accuracy: 0.8895 - val_loss: 0.3018 - val_accuracy: 0.8780 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "1520/1520 [==============================] - 454s 299ms/step - loss: 0.2568 - accuracy: 0.8934 - val_loss: 0.3088 - val_accuracy: 0.8717 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "1520/1520 [==============================] - 448s 295ms/step - loss: 0.2512 - accuracy: 0.8959 - val_loss: 0.2937 - val_accuracy: 0.8737 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "1520/1520 [==============================] - 453s 298ms/step - loss: 0.2403 - accuracy: 0.9015 - val_loss: 0.3085 - val_accuracy: 0.8755 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "1520/1520 [==============================] - 483s 318ms/step - loss: 0.2336 - accuracy: 0.9042 - val_loss: 0.3265 - val_accuracy: 0.8691 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "1520/1520 [==============================] - 503s 331ms/step - loss: 0.2248 - accuracy: 0.9068 - val_loss: 0.3111 - val_accuracy: 0.8742 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "1520/1520 [==============================] - 443s 291ms/step - loss: 0.2197 - accuracy: 0.9108 - val_loss: 0.3012 - val_accuracy: 0.8763 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "1520/1520 [==============================] - ETA: 0s - loss: 0.2111 - accuracy: 0.9147\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "1520/1520 [==============================] - 437s 288ms/step - loss: 0.2111 - accuracy: 0.9147 - val_loss: 0.3115 - val_accuracy: 0.8699 - lr: 0.0010\n",
      "Epoch 19: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16660750ac0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_count_matrix = count_matrix[:len(train_data)]\n",
    "dense_val_count_matrix = count_matrix[len(train_data):len(train_data) + len(val_data)]\n",
    "\n",
    "# Assuming train_data['label'] and val_data['label'] are Pandas Series, convert them to arrays\n",
    "train_labels = train_data['label'].values\n",
    "val_labels = val_data['label'].values\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1)\n",
    "\n",
    "# Fit the model using NumPy arrays with callbacks\n",
    "model.fit(dense_count_matrix, train_labels, \n",
    "          epochs=100, batch_size=32, \n",
    "          validation_data=(dense_val_count_matrix, val_labels),\n",
    "          callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191/191 [==============================] - 17s 81ms/step - loss: 0.3118 - accuracy: 0.8744\n",
      "Test accuracy: 0.8743627667427063\n"
     ]
    }
   ],
   "source": [
    "text_test_preprocessed = text_test.apply(preprocess_text)\n",
    "test_count_matrix = count_vectorizer.transform(text_test_preprocessed)\n",
    "dense_test_count_matrix = svd.transform(test_count_matrix)\n",
    "test_labels = test_data['label'].values\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "test_loss, test_accuracy = model.evaluate(dense_test_count_matrix, test_labels)\n",
    "print(f'Test accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('88Acc_64L_32L_16D_100E_32B.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy * lengthTestData"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
