{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LEGION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LEGION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LEGION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_data = pd.read_csv('train.csv')\n",
    "val_data = pd.read_csv('val.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "lengthTestData = len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine text data for preprocessing\n",
    "text = pd.concat([train_data['text'], val_data['text']], ignore_index=True)\n",
    "text_test = test_data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenization and lowercase\n",
    "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]  # Remove stopwords and lemmatize\n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessed = text.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        beirut ( reuters ) - iran military chief met s...\n",
      "1        hanoi ( reuters ) - top u.s. envoy began two-d...\n",
      "2        ( reuters ) - four u.s. senator asked senate j...\n",
      "3        first read morning briefing meet press nbc pol...\n",
      "4        cairo ( reuters ) - six month egypt election ,...\n",
      "                               ...                        \n",
      "54714    lack oversight prof donald trump totally unfit...\n",
      "54715    tucker carlson responded espn anchor calling p...\n",
      "54716    getting something nothing rage president profe...\n",
      "54717    black emanuelle fixed 1976. attila speaking eu...\n",
      "54718    chaos broke legal american illegal alien clash...\n",
      "Name: text, Length: 54719, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        beirut (reuters) - iran s military chief met w...\n",
      "1        hanoi (reuters) - a top u.s. envoy began a two...\n",
      "2        (reuters) - four u.s. senators have asked the ...\n",
      "3        first read is a morning briefing from meet the...\n",
      "4        cairo (reuters) - six months before egypt s el...\n",
      "                               ...                        \n",
      "54714    this lack of oversight proves that donald trum...\n",
      "54715    tucker carlson responded to an espn anchor cal...\n",
      "54716    because getting something for nothing is all t...\n",
      "54717    black emanuelle fixed all that in 1976. attila...\n",
      "54718    chaos broke out after legal americans and ille...\n",
      "Name: text, Length: 54719, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF vectorizer without specifying max_features\n",
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the preprocessed text data\n",
    "count_matrix = count_vectorizer.fit_transform(text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169079\n"
     ]
    }
   ],
   "source": [
    "# Get the number of unique tokens\n",
    "num_unique_tokens = len(count_vectorizer.get_feature_names_out())\n",
    "print(num_unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize TF-IDF vectorizer with the determined max_features\n",
    "count_vectorizer = CountVectorizer(max_features=num_unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit and transform the text data again with the updated max_features\n",
    "count_matrix = count_vectorizer.fit_transform(text_preprocessed)\n",
    "\n",
    "# Convert the TF-IDF matrix to a CSR (Compressed Sparse Row) matrix for efficient row-wise operations\n",
    "csr_count_matrix = csr_matrix(count_matrix)\n",
    "\n",
    "# Find the row index with the maximum number of filled values\n",
    "max_features_row_index = csr_count_matrix.getnnz(axis=1).argmax()\n",
    "\n",
    "# Get the number of features in the document with the most filled values\n",
    "max_features = csr_count_matrix[max_features_row_index].count_nonzero()\n",
    "\n",
    "svd = TruncatedSVD(n_components=int(max_features*0.3))\n",
    "count_matrix = svd.fit_transform(count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dense_tfidf_matrix = tfidf_matrix[:len(train_data)]\n",
    "#dense_val_tfidf_matrix = tfidf_matrix[len(train_data):len(train_data) + len(val_data)]\n",
    "\n",
    "# Merging the Validation and Training Data into one for a larger training dataset.\n",
    "#dense_count_matrix = count_matrix[:len(train_data) + len(val_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert them into Arrays\n",
    "train_labels = train_data['label'].values\n",
    "val_labels = val_data['label'].values\n",
    "\n",
    "dense_labels = np.concatenate((train_data['label'].values, val_data['label'].values), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the MLP model\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(600, 10), max_iter=300, activation='relu', solver='adam', learning_rate='adaptive', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.22311589\n",
      "Iteration 2, loss = 0.10142026\n",
      "Iteration 3, loss = 0.06292075\n",
      "Iteration 4, loss = 0.03624660\n",
      "Iteration 5, loss = 0.03223370\n",
      "Iteration 6, loss = 0.02099332\n",
      "Iteration 7, loss = 0.01396396\n",
      "Iteration 8, loss = 0.01675547\n",
      "Iteration 9, loss = 0.01124450\n",
      "Iteration 10, loss = 0.01344241\n",
      "Iteration 11, loss = 0.01000686\n",
      "Iteration 12, loss = 0.00841386\n",
      "Iteration 13, loss = 0.01061452\n",
      "Iteration 14, loss = 0.01473770\n",
      "Iteration 15, loss = 0.01117886\n",
      "Iteration 16, loss = 0.00748819\n",
      "Iteration 17, loss = 0.00575764\n",
      "Iteration 18, loss = 0.00276349\n",
      "Iteration 19, loss = 0.00390956\n",
      "Iteration 20, loss = 0.00953722\n",
      "Iteration 21, loss = 0.01518913\n",
      "Iteration 22, loss = 0.00664080\n",
      "Iteration 23, loss = 0.00388045\n",
      "Iteration 24, loss = 0.00305541\n",
      "Iteration 25, loss = 0.00296733\n",
      "Iteration 26, loss = 0.00193372\n",
      "Iteration 27, loss = 0.00165110\n",
      "Iteration 28, loss = 0.00164526\n",
      "Iteration 29, loss = 0.00155100\n",
      "Iteration 30, loss = 0.00157549\n",
      "Iteration 31, loss = 0.00157219\n",
      "Iteration 32, loss = 0.00148681\n",
      "Iteration 33, loss = 0.00147228\n",
      "Iteration 34, loss = 0.00145937\n",
      "Iteration 35, loss = 0.00144770\n",
      "Iteration 36, loss = 0.00144296\n",
      "Iteration 37, loss = 0.00142874\n",
      "Iteration 38, loss = 0.00141463\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.21407866\n",
      "Iteration 2, loss = 0.09858611\n",
      "Iteration 3, loss = 0.05704863\n",
      "Iteration 4, loss = 0.03255130\n",
      "Iteration 5, loss = 0.01770230\n",
      "Iteration 6, loss = 0.01567621\n",
      "Iteration 7, loss = 0.02736340\n",
      "Iteration 8, loss = 0.01744141\n",
      "Iteration 9, loss = 0.01167577\n",
      "Iteration 10, loss = 0.00651363\n",
      "Iteration 11, loss = 0.01730565\n",
      "Iteration 12, loss = 0.01242327\n",
      "Iteration 13, loss = 0.00937207\n",
      "Iteration 14, loss = 0.00468114\n",
      "Iteration 15, loss = 0.00335848\n",
      "Iteration 16, loss = 0.00160684\n",
      "Iteration 17, loss = 0.00141471\n",
      "Iteration 18, loss = 0.00127324\n",
      "Iteration 19, loss = 0.00159721\n",
      "Iteration 20, loss = 0.00127039\n",
      "Iteration 21, loss = 0.00154323\n",
      "Iteration 22, loss = 0.00109134\n",
      "Iteration 23, loss = 0.00098613\n",
      "Iteration 24, loss = 0.00106220\n",
      "Iteration 25, loss = 0.00101874\n",
      "Iteration 26, loss = 0.00096422\n",
      "Iteration 27, loss = 0.04704022\n",
      "Iteration 28, loss = 0.02446538\n",
      "Iteration 29, loss = 0.00881461\n",
      "Iteration 30, loss = 0.00397218\n",
      "Iteration 31, loss = 0.00327538\n",
      "Iteration 32, loss = 0.00242748\n",
      "Iteration 33, loss = 0.00147178\n",
      "Iteration 34, loss = 0.00137528\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.21820163\n",
      "Iteration 2, loss = 0.09498087\n",
      "Iteration 3, loss = 0.05575437\n",
      "Iteration 4, loss = 0.03044646\n",
      "Iteration 5, loss = 0.02066660\n",
      "Iteration 6, loss = 0.01367355\n",
      "Iteration 7, loss = 0.01609588\n",
      "Iteration 8, loss = 0.02788726\n",
      "Iteration 9, loss = 0.02037635\n",
      "Iteration 10, loss = 0.01639377\n",
      "Iteration 11, loss = 0.00816170\n",
      "Iteration 12, loss = 0.00449980\n",
      "Iteration 13, loss = 0.00393653\n",
      "Iteration 14, loss = 0.00184700\n",
      "Iteration 15, loss = 0.00183166\n",
      "Iteration 16, loss = 0.00143168\n",
      "Iteration 17, loss = 0.00133926\n",
      "Iteration 18, loss = 0.00142903\n",
      "Iteration 19, loss = 0.00126148\n",
      "Iteration 20, loss = 0.00114554\n",
      "Iteration 21, loss = 0.00116396\n",
      "Iteration 22, loss = 0.00114470\n",
      "Iteration 23, loss = 0.00710521\n",
      "Iteration 24, loss = 0.05504337\n",
      "Iteration 25, loss = 0.01909720\n",
      "Iteration 26, loss = 0.00803092\n",
      "Iteration 27, loss = 0.00280112\n",
      "Iteration 28, loss = 0.00179999\n",
      "Iteration 29, loss = 0.00154202\n",
      "Iteration 30, loss = 0.00140555\n",
      "Iteration 31, loss = 0.00135531\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.21330207\n",
      "Iteration 2, loss = 0.09440442\n",
      "Iteration 3, loss = 0.05611419\n",
      "Iteration 4, loss = 0.03183787\n",
      "Iteration 5, loss = 0.02153795\n",
      "Iteration 6, loss = 0.01740684\n",
      "Iteration 7, loss = 0.02178888\n",
      "Iteration 8, loss = 0.02018622\n",
      "Iteration 9, loss = 0.01232975\n",
      "Iteration 10, loss = 0.01179455\n",
      "Iteration 11, loss = 0.00807971\n",
      "Iteration 12, loss = 0.00645414\n",
      "Iteration 13, loss = 0.00927535\n",
      "Iteration 14, loss = 0.01189136\n",
      "Iteration 15, loss = 0.01006459\n",
      "Iteration 16, loss = 0.00865170\n",
      "Iteration 17, loss = 0.00556954\n",
      "Iteration 18, loss = 0.01151007\n",
      "Iteration 19, loss = 0.01433020\n",
      "Iteration 20, loss = 0.00518631\n",
      "Iteration 21, loss = 0.00538650\n",
      "Iteration 22, loss = 0.00215581\n",
      "Iteration 23, loss = 0.00404640\n",
      "Iteration 24, loss = 0.00342671\n",
      "Iteration 25, loss = 0.00187284\n",
      "Iteration 26, loss = 0.00218909\n",
      "Iteration 27, loss = 0.00733763\n",
      "Iteration 28, loss = 0.02224727\n",
      "Iteration 29, loss = 0.00758766\n",
      "Iteration 30, loss = 0.00390006\n",
      "Iteration 31, loss = 0.00288883\n",
      "Iteration 32, loss = 0.00196429\n",
      "Iteration 33, loss = 0.00157873\n",
      "Iteration 34, loss = 0.00147090\n",
      "Iteration 35, loss = 0.00143139\n",
      "Iteration 36, loss = 0.00146157\n",
      "Iteration 37, loss = 0.00139268\n",
      "Iteration 38, loss = 0.00143376\n",
      "Iteration 39, loss = 0.00140406\n",
      "Iteration 40, loss = 0.00136352\n",
      "Iteration 41, loss = 0.00134151\n",
      "Iteration 42, loss = 0.00133306\n",
      "Iteration 43, loss = 0.00202896\n",
      "Iteration 44, loss = 0.03461092\n",
      "Iteration 45, loss = 0.01202286\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22000212\n",
      "Iteration 2, loss = 0.09704483\n",
      "Iteration 3, loss = 0.05474545\n",
      "Iteration 4, loss = 0.03508453\n",
      "Iteration 5, loss = 0.02980542\n",
      "Iteration 6, loss = 0.01721702\n",
      "Iteration 7, loss = 0.01790102\n",
      "Iteration 8, loss = 0.02128505\n",
      "Iteration 9, loss = 0.01101822\n",
      "Iteration 10, loss = 0.01677151\n",
      "Iteration 11, loss = 0.02025206\n",
      "Iteration 12, loss = 0.00858986\n",
      "Iteration 13, loss = 0.00877092\n",
      "Iteration 14, loss = 0.00983150\n",
      "Iteration 15, loss = 0.00385187\n",
      "Iteration 16, loss = 0.00558162\n",
      "Iteration 17, loss = 0.00416976\n",
      "Iteration 18, loss = 0.01020828\n",
      "Iteration 19, loss = 0.00890715\n",
      "Iteration 20, loss = 0.01191532\n",
      "Iteration 21, loss = 0.00759932\n",
      "Iteration 22, loss = 0.00384162\n",
      "Iteration 23, loss = 0.00311162\n",
      "Iteration 24, loss = 0.00199107\n",
      "Iteration 25, loss = 0.00387498\n",
      "Iteration 26, loss = 0.01616056\n",
      "Iteration 27, loss = 0.01233565\n",
      "Iteration 28, loss = 0.00763592\n",
      "Iteration 29, loss = 0.00547281\n",
      "Iteration 30, loss = 0.00469879\n",
      "Iteration 31, loss = 0.00348072\n",
      "Iteration 32, loss = 0.00752308\n",
      "Iteration 33, loss = 0.00532212\n",
      "Iteration 34, loss = 0.00290719\n",
      "Iteration 35, loss = 0.00202214\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Cross-validation scores: [0.95741959 0.95458699 0.95458699 0.95440424 0.9553139 ]\n",
      "Average CV accuracy: 0.9552623412627795\n"
     ]
    }
   ],
   "source": [
    "#CROSS FOLD VALIDATION, DELETE IF NECESSARY\n",
    "\n",
    "# Initialize KFold with the desired number of folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(mlp_model, count_matrix, dense_labels, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Average CV accuracy:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.20356884\n",
      "Iteration 2, loss = 0.09563947\n",
      "Iteration 3, loss = 0.05739103\n",
      "Iteration 4, loss = 0.03368252\n",
      "Iteration 5, loss = 0.02742316\n",
      "Iteration 6, loss = 0.01518287\n",
      "Iteration 7, loss = 0.02158856\n",
      "Iteration 8, loss = 0.02367353\n",
      "Iteration 9, loss = 0.01558794\n",
      "Iteration 10, loss = 0.01183056\n",
      "Iteration 11, loss = 0.01122376\n",
      "Iteration 12, loss = 0.00848485\n",
      "Iteration 13, loss = 0.00533786\n",
      "Iteration 14, loss = 0.00463352\n",
      "Iteration 15, loss = 0.01760794\n",
      "Iteration 16, loss = 0.01172470\n",
      "Iteration 17, loss = 0.00606997\n",
      "Iteration 18, loss = 0.00442482\n",
      "Iteration 19, loss = 0.00684529\n",
      "Iteration 20, loss = 0.00858063\n",
      "Iteration 21, loss = 0.01334403\n",
      "Iteration 22, loss = 0.00677551\n",
      "Iteration 23, loss = 0.00568633\n",
      "Iteration 24, loss = 0.01472434\n",
      "Iteration 25, loss = 0.00539765\n",
      "Iteration 26, loss = 0.00426481\n",
      "Iteration 27, loss = 0.00196790\n",
      "Iteration 28, loss = 0.00249798\n",
      "Iteration 29, loss = 0.00362267\n",
      "Iteration 30, loss = 0.00495624\n",
      "Iteration 31, loss = 0.00737764\n",
      "Iteration 32, loss = 0.00953757\n",
      "Iteration 33, loss = 0.00999419\n",
      "Iteration 34, loss = 0.00636062\n",
      "Iteration 35, loss = 0.00352352\n",
      "Iteration 36, loss = 0.00347995\n",
      "Iteration 37, loss = 0.00497138\n",
      "Iteration 38, loss = 0.00714510\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(600, 10), learning_rate=&#x27;adaptive&#x27;,\n",
       "              max_iter=300, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(600, 10), learning_rate=&#x27;adaptive&#x27;,\n",
       "              max_iter=300, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(600, 10), learning_rate='adaptive',\n",
       "              max_iter=300, verbose=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_model.fit(count_matrix, dense_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9542838348955763\n"
     ]
    }
   ],
   "source": [
    "text_test_preprocessed = text_test.apply(preprocess_text)\n",
    "test_count_matrix = count_vectorizer.transform(text_test_preprocessed)\n",
    "dense_test_count_matrix = svd.transform(test_count_matrix)\n",
    "test_labels = test_data['label'].values\n",
    "\n",
    "test_predictions = mlp_model.predict(dense_test_count_matrix)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f'Test accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96      3420\n",
      "           1       0.95      0.94      0.95      2661\n",
      "\n",
      "    accuracy                           0.95      6081\n",
      "   macro avg       0.95      0.95      0.95      6081\n",
      "weighted avg       0.95      0.95      0.95      6081\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_labels, test_predictions)\n",
    "print(report)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mlp_model_cv.joblib']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "dump(mlp_model, 'mlp_model_cv.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6081\n"
     ]
    }
   ],
   "source": [
    "print(lengthTestData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5803.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy*lengthTestData"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
