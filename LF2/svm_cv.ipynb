{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LEGION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LEGION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LEGION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_data = pd.read_csv('train.csv')\n",
    "val_data = pd.read_csv('val.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "lengthTestData = len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine text data for preprocessing\n",
    "text = pd.concat([train_data['text'], val_data['text']], ignore_index=True)\n",
    "text_test = test_data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "linguistic_features = pd.concat([train_data.iloc[:,-31:], val_data.iloc[:,-31:]], ignore_index=True)\n",
    "test_linguistic_features = test_data.iloc[:,-31:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words_per_Sentence</th>\n",
       "      <th>Percentage_First_Person_Singular</th>\n",
       "      <th>Percentage_Third_Person</th>\n",
       "      <th>Percentage_Exclusive</th>\n",
       "      <th>Percentage_Negation</th>\n",
       "      <th>Percentage_Causation</th>\n",
       "      <th>Percentage_Sense</th>\n",
       "      <th>Percentage_PositiveEmo</th>\n",
       "      <th>Percentage_NegativeEmo</th>\n",
       "      <th>Percentage_AffectiveTerms</th>\n",
       "      <th>...</th>\n",
       "      <th>Percentage_PastVerb</th>\n",
       "      <th>Percentage_PresentVerb</th>\n",
       "      <th>Percentage_FutureVerb</th>\n",
       "      <th>Percentage_Article</th>\n",
       "      <th>Percentage_Pronoun</th>\n",
       "      <th>Percentage_Conjunction</th>\n",
       "      <th>Lexical_Diversity</th>\n",
       "      <th>Emotiveness</th>\n",
       "      <th>Rate_of_Adjectives_Adverbs</th>\n",
       "      <th>Flesch_Kincaid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.930502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.702703</td>\n",
       "      <td>5.405405</td>\n",
       "      <td>3.474903</td>\n",
       "      <td>3.474903</td>\n",
       "      <td>6.949807</td>\n",
       "      <td>...</td>\n",
       "      <td>3.861004</td>\n",
       "      <td>4.633205</td>\n",
       "      <td>0.772201</td>\n",
       "      <td>5.019305</td>\n",
       "      <td>4.247104</td>\n",
       "      <td>5.405405</td>\n",
       "      <td>0.548263</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>14.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.735562</td>\n",
       "      <td>1.519757</td>\n",
       "      <td>0.607903</td>\n",
       "      <td>1.519757</td>\n",
       "      <td>4.863222</td>\n",
       "      <td>2.735562</td>\n",
       "      <td>0.607903</td>\n",
       "      <td>3.343465</td>\n",
       "      <td>...</td>\n",
       "      <td>2.127660</td>\n",
       "      <td>2.127660</td>\n",
       "      <td>0.911854</td>\n",
       "      <td>7.294833</td>\n",
       "      <td>5.471125</td>\n",
       "      <td>5.167173</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.296610</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>13.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.416667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.191781</td>\n",
       "      <td>0.821918</td>\n",
       "      <td>0.547945</td>\n",
       "      <td>1.095890</td>\n",
       "      <td>6.027397</td>\n",
       "      <td>3.013699</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.013699</td>\n",
       "      <td>...</td>\n",
       "      <td>3.835616</td>\n",
       "      <td>3.013699</td>\n",
       "      <td>1.095890</td>\n",
       "      <td>8.219178</td>\n",
       "      <td>4.931507</td>\n",
       "      <td>3.561644</td>\n",
       "      <td>0.495890</td>\n",
       "      <td>0.246479</td>\n",
       "      <td>0.095890</td>\n",
       "      <td>11.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.254545</td>\n",
       "      <td>0.085543</td>\n",
       "      <td>4.790419</td>\n",
       "      <td>1.710864</td>\n",
       "      <td>1.197605</td>\n",
       "      <td>2.138580</td>\n",
       "      <td>3.079555</td>\n",
       "      <td>2.053037</td>\n",
       "      <td>1.796407</td>\n",
       "      <td>3.849444</td>\n",
       "      <td>...</td>\n",
       "      <td>2.138580</td>\n",
       "      <td>4.790419</td>\n",
       "      <td>0.427716</td>\n",
       "      <td>6.843456</td>\n",
       "      <td>8.297690</td>\n",
       "      <td>5.560308</td>\n",
       "      <td>0.437981</td>\n",
       "      <td>0.415254</td>\n",
       "      <td>0.125749</td>\n",
       "      <td>9.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.181818</td>\n",
       "      <td>1.636364</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>2.181818</td>\n",
       "      <td>6.181818</td>\n",
       "      <td>3.545455</td>\n",
       "      <td>1.727273</td>\n",
       "      <td>5.272727</td>\n",
       "      <td>...</td>\n",
       "      <td>2.818182</td>\n",
       "      <td>5.818182</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>5.909091</td>\n",
       "      <td>4.727273</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.273171</td>\n",
       "      <td>0.101818</td>\n",
       "      <td>10.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54714</th>\n",
       "      <td>56.166667</td>\n",
       "      <td>0.296736</td>\n",
       "      <td>4.154303</td>\n",
       "      <td>1.186944</td>\n",
       "      <td>1.186944</td>\n",
       "      <td>0.593472</td>\n",
       "      <td>5.341246</td>\n",
       "      <td>2.670623</td>\n",
       "      <td>2.077151</td>\n",
       "      <td>4.747774</td>\n",
       "      <td>...</td>\n",
       "      <td>2.373887</td>\n",
       "      <td>5.044510</td>\n",
       "      <td>0.296736</td>\n",
       "      <td>8.011869</td>\n",
       "      <td>7.715134</td>\n",
       "      <td>4.154303</td>\n",
       "      <td>0.554896</td>\n",
       "      <td>0.274336</td>\n",
       "      <td>0.091988</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54715</th>\n",
       "      <td>22.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.183406</td>\n",
       "      <td>0.873362</td>\n",
       "      <td>0.436681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.240175</td>\n",
       "      <td>1.310044</td>\n",
       "      <td>3.493450</td>\n",
       "      <td>4.803493</td>\n",
       "      <td>...</td>\n",
       "      <td>3.056769</td>\n",
       "      <td>3.493450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.113537</td>\n",
       "      <td>4.366812</td>\n",
       "      <td>2.620087</td>\n",
       "      <td>0.620087</td>\n",
       "      <td>0.271605</td>\n",
       "      <td>0.096070</td>\n",
       "      <td>10.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54716</th>\n",
       "      <td>18.095238</td>\n",
       "      <td>1.842105</td>\n",
       "      <td>5.263158</td>\n",
       "      <td>2.105263</td>\n",
       "      <td>1.052632</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>5.789474</td>\n",
       "      <td>3.684211</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>4.736842</td>\n",
       "      <td>...</td>\n",
       "      <td>5.789474</td>\n",
       "      <td>4.736842</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>4.736842</td>\n",
       "      <td>18.947368</td>\n",
       "      <td>5.789474</td>\n",
       "      <td>0.484211</td>\n",
       "      <td>0.362832</td>\n",
       "      <td>0.107895</td>\n",
       "      <td>6.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54717</th>\n",
       "      <td>16.611354</td>\n",
       "      <td>1.156677</td>\n",
       "      <td>2.970557</td>\n",
       "      <td>2.602524</td>\n",
       "      <td>1.445846</td>\n",
       "      <td>1.393270</td>\n",
       "      <td>2.628812</td>\n",
       "      <td>2.523659</td>\n",
       "      <td>1.919033</td>\n",
       "      <td>4.468980</td>\n",
       "      <td>...</td>\n",
       "      <td>3.548896</td>\n",
       "      <td>5.783386</td>\n",
       "      <td>0.578339</td>\n",
       "      <td>6.887487</td>\n",
       "      <td>10.778128</td>\n",
       "      <td>6.834911</td>\n",
       "      <td>0.289432</td>\n",
       "      <td>0.457865</td>\n",
       "      <td>0.128549</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54718</th>\n",
       "      <td>28.111111</td>\n",
       "      <td>0.988142</td>\n",
       "      <td>4.743083</td>\n",
       "      <td>1.185771</td>\n",
       "      <td>1.185771</td>\n",
       "      <td>1.383399</td>\n",
       "      <td>4.743083</td>\n",
       "      <td>1.383399</td>\n",
       "      <td>1.581028</td>\n",
       "      <td>2.964427</td>\n",
       "      <td>...</td>\n",
       "      <td>5.138340</td>\n",
       "      <td>4.347826</td>\n",
       "      <td>0.395257</td>\n",
       "      <td>5.731225</td>\n",
       "      <td>8.695652</td>\n",
       "      <td>6.324111</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.261780</td>\n",
       "      <td>0.098814</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54719 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Words_per_Sentence  Percentage_First_Person_Singular  \\\n",
       "0               28.777778                          0.000000   \n",
       "1               32.900000                          0.000000   \n",
       "2               30.416667                          0.000000   \n",
       "3               21.254545                          0.085543   \n",
       "4               24.444444                          0.000000   \n",
       "...                   ...                               ...   \n",
       "54714           56.166667                          0.296736   \n",
       "54715           22.900000                          0.000000   \n",
       "54716           18.095238                          1.842105   \n",
       "54717           16.611354                          1.156677   \n",
       "54718           28.111111                          0.988142   \n",
       "\n",
       "       Percentage_Third_Person  Percentage_Exclusive  Percentage_Negation  \\\n",
       "0                     1.930502              0.000000             0.000000   \n",
       "1                     2.735562              1.519757             0.607903   \n",
       "2                     2.191781              0.821918             0.547945   \n",
       "3                     4.790419              1.710864             1.197605   \n",
       "4                     3.181818              1.636364             0.545455   \n",
       "...                        ...                   ...                  ...   \n",
       "54714                 4.154303              1.186944             1.186944   \n",
       "54715                 2.183406              0.873362             0.436681   \n",
       "54716                 5.263158              2.105263             1.052632   \n",
       "54717                 2.970557              2.602524             1.445846   \n",
       "54718                 4.743083              1.185771             1.185771   \n",
       "\n",
       "       Percentage_Causation  Percentage_Sense  Percentage_PositiveEmo  \\\n",
       "0                  2.702703          5.405405                3.474903   \n",
       "1                  1.519757          4.863222                2.735562   \n",
       "2                  1.095890          6.027397                3.013699   \n",
       "3                  2.138580          3.079555                2.053037   \n",
       "4                  2.181818          6.181818                3.545455   \n",
       "...                     ...               ...                     ...   \n",
       "54714              0.593472          5.341246                2.670623   \n",
       "54715              0.000000          5.240175                1.310044   \n",
       "54716              1.315789          5.789474                3.684211   \n",
       "54717              1.393270          2.628812                2.523659   \n",
       "54718              1.383399          4.743083                1.383399   \n",
       "\n",
       "       Percentage_NegativeEmo  Percentage_AffectiveTerms  ...  \\\n",
       "0                    3.474903                   6.949807  ...   \n",
       "1                    0.607903                   3.343465  ...   \n",
       "2                    0.000000                   3.013699  ...   \n",
       "3                    1.796407                   3.849444  ...   \n",
       "4                    1.727273                   5.272727  ...   \n",
       "...                       ...                        ...  ...   \n",
       "54714                2.077151                   4.747774  ...   \n",
       "54715                3.493450                   4.803493  ...   \n",
       "54716                0.789474                   4.736842  ...   \n",
       "54717                1.919033                   4.468980  ...   \n",
       "54718                1.581028                   2.964427  ...   \n",
       "\n",
       "       Percentage_PastVerb  Percentage_PresentVerb  Percentage_FutureVerb  \\\n",
       "0                 3.861004                4.633205               0.772201   \n",
       "1                 2.127660                2.127660               0.911854   \n",
       "2                 3.835616                3.013699               1.095890   \n",
       "3                 2.138580                4.790419               0.427716   \n",
       "4                 2.818182                5.818182               0.818182   \n",
       "...                    ...                     ...                    ...   \n",
       "54714             2.373887                5.044510               0.296736   \n",
       "54715             3.056769                3.493450               0.000000   \n",
       "54716             5.789474                4.736842               0.263158   \n",
       "54717             3.548896                5.783386               0.578339   \n",
       "54718             5.138340                4.347826               0.395257   \n",
       "\n",
       "       Percentage_Article  Percentage_Pronoun  Percentage_Conjunction  \\\n",
       "0                5.019305            4.247104                5.405405   \n",
       "1                7.294833            5.471125                5.167173   \n",
       "2                8.219178            4.931507                3.561644   \n",
       "3                6.843456            8.297690                5.560308   \n",
       "4                9.000000            5.909091                4.727273   \n",
       "...                   ...                 ...                     ...   \n",
       "54714            8.011869            7.715134                4.154303   \n",
       "54715            6.113537            4.366812                2.620087   \n",
       "54716            4.736842           18.947368                5.789474   \n",
       "54717            6.887487           10.778128                6.834911   \n",
       "54718            5.731225            8.695652                6.324111   \n",
       "\n",
       "       Lexical_Diversity  Emotiveness  Rate_of_Adjectives_Adverbs  \\\n",
       "0               0.548263     0.247059                    0.081081   \n",
       "1               0.595745     0.296610                    0.106383   \n",
       "2               0.495890     0.246479                    0.095890   \n",
       "3               0.437981     0.415254                    0.125749   \n",
       "4               0.440000     0.273171                    0.101818   \n",
       "...                  ...          ...                         ...   \n",
       "54714           0.554896     0.274336                    0.091988   \n",
       "54715           0.620087     0.271605                    0.096070   \n",
       "54716           0.484211     0.362832                    0.107895   \n",
       "54717           0.289432     0.457865                    0.128549   \n",
       "54718           0.521739     0.261780                    0.098814   \n",
       "\n",
       "       Flesch_Kincaid  \n",
       "0                14.1  \n",
       "1                13.5  \n",
       "2                11.8  \n",
       "3                 9.9  \n",
       "4                10.8  \n",
       "...               ...  \n",
       "54714            12.0  \n",
       "54715            10.7  \n",
       "54716             6.8  \n",
       "54717             8.2  \n",
       "54718            10.0  \n",
       "\n",
       "[54719 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linguistic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words_per_Sentence</th>\n",
       "      <th>Percentage_First_Person_Singular</th>\n",
       "      <th>Percentage_Third_Person</th>\n",
       "      <th>Percentage_Exclusive</th>\n",
       "      <th>Percentage_Negation</th>\n",
       "      <th>Percentage_Causation</th>\n",
       "      <th>Percentage_Sense</th>\n",
       "      <th>Percentage_PositiveEmo</th>\n",
       "      <th>Percentage_NegativeEmo</th>\n",
       "      <th>Percentage_AffectiveTerms</th>\n",
       "      <th>...</th>\n",
       "      <th>Percentage_PastVerb</th>\n",
       "      <th>Percentage_PresentVerb</th>\n",
       "      <th>Percentage_FutureVerb</th>\n",
       "      <th>Percentage_Article</th>\n",
       "      <th>Percentage_Pronoun</th>\n",
       "      <th>Percentage_Conjunction</th>\n",
       "      <th>Lexical_Diversity</th>\n",
       "      <th>Emotiveness</th>\n",
       "      <th>Rate_of_Adjectives_Adverbs</th>\n",
       "      <th>Flesch_Kincaid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.393617</td>\n",
       "      <td>1.595745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.393617</td>\n",
       "      <td>7.446809</td>\n",
       "      <td>2.393617</td>\n",
       "      <td>1.861702</td>\n",
       "      <td>4.255319</td>\n",
       "      <td>...</td>\n",
       "      <td>6.648936</td>\n",
       "      <td>3.191489</td>\n",
       "      <td>0.797872</td>\n",
       "      <td>11.170213</td>\n",
       "      <td>5.585106</td>\n",
       "      <td>5.585106</td>\n",
       "      <td>0.486702</td>\n",
       "      <td>0.345133</td>\n",
       "      <td>0.103723</td>\n",
       "      <td>9.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>2.142857</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>2.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>7.857143</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>2.142857</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.285714</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.369863</td>\n",
       "      <td>1.369863</td>\n",
       "      <td>1.369863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.219178</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.109589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.369863</td>\n",
       "      <td>6.849315</td>\n",
       "      <td>1.369863</td>\n",
       "      <td>1.369863</td>\n",
       "      <td>0.698630</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.054795</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.466667</td>\n",
       "      <td>0.404531</td>\n",
       "      <td>3.559871</td>\n",
       "      <td>0.566343</td>\n",
       "      <td>0.485437</td>\n",
       "      <td>1.779935</td>\n",
       "      <td>3.478964</td>\n",
       "      <td>2.831715</td>\n",
       "      <td>1.699029</td>\n",
       "      <td>4.530744</td>\n",
       "      <td>...</td>\n",
       "      <td>4.126214</td>\n",
       "      <td>2.993528</td>\n",
       "      <td>0.485437</td>\n",
       "      <td>8.656958</td>\n",
       "      <td>6.877023</td>\n",
       "      <td>4.773463</td>\n",
       "      <td>0.411812</td>\n",
       "      <td>0.285360</td>\n",
       "      <td>0.093042</td>\n",
       "      <td>11.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.278689</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.278689</td>\n",
       "      <td>6.557377</td>\n",
       "      <td>2.459016</td>\n",
       "      <td>2.459016</td>\n",
       "      <td>4.918033</td>\n",
       "      <td>...</td>\n",
       "      <td>6.557377</td>\n",
       "      <td>3.278689</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>8.196721</td>\n",
       "      <td>4.918033</td>\n",
       "      <td>2.459016</td>\n",
       "      <td>0.672131</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>0.139344</td>\n",
       "      <td>12.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6076</th>\n",
       "      <td>26.708333</td>\n",
       "      <td>0.468019</td>\n",
       "      <td>4.680187</td>\n",
       "      <td>2.184087</td>\n",
       "      <td>0.936037</td>\n",
       "      <td>1.560062</td>\n",
       "      <td>1.872075</td>\n",
       "      <td>1.560062</td>\n",
       "      <td>2.808112</td>\n",
       "      <td>4.368175</td>\n",
       "      <td>...</td>\n",
       "      <td>3.432137</td>\n",
       "      <td>4.056162</td>\n",
       "      <td>1.092044</td>\n",
       "      <td>4.368175</td>\n",
       "      <td>11.076443</td>\n",
       "      <td>6.240250</td>\n",
       "      <td>0.436817</td>\n",
       "      <td>0.325472</td>\n",
       "      <td>0.107644</td>\n",
       "      <td>10.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6077</th>\n",
       "      <td>32.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.834862</td>\n",
       "      <td>0.305810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.587156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.504587</td>\n",
       "      <td>5.504587</td>\n",
       "      <td>...</td>\n",
       "      <td>1.529052</td>\n",
       "      <td>4.892966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.785933</td>\n",
       "      <td>3.975535</td>\n",
       "      <td>4.892966</td>\n",
       "      <td>0.483180</td>\n",
       "      <td>0.405405</td>\n",
       "      <td>0.137615</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6078</th>\n",
       "      <td>25.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.427083</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>2.083333</td>\n",
       "      <td>4.687500</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.822917</td>\n",
       "      <td>5.989583</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>6.510417</td>\n",
       "      <td>5.468750</td>\n",
       "      <td>5.989583</td>\n",
       "      <td>0.476562</td>\n",
       "      <td>0.297101</td>\n",
       "      <td>0.106771</td>\n",
       "      <td>11.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6079</th>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>2.380952</td>\n",
       "      <td>1.190476</td>\n",
       "      <td>1.587302</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>...</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>1.984127</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>1.984127</td>\n",
       "      <td>0.575397</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>26.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6080</th>\n",
       "      <td>17.125000</td>\n",
       "      <td>0.729927</td>\n",
       "      <td>8.029197</td>\n",
       "      <td>5.109489</td>\n",
       "      <td>0.729927</td>\n",
       "      <td>2.919708</td>\n",
       "      <td>5.109489</td>\n",
       "      <td>5.839416</td>\n",
       "      <td>2.919708</td>\n",
       "      <td>8.759124</td>\n",
       "      <td>...</td>\n",
       "      <td>2.919708</td>\n",
       "      <td>5.839416</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.299270</td>\n",
       "      <td>13.868613</td>\n",
       "      <td>5.839416</td>\n",
       "      <td>0.671533</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>0.116788</td>\n",
       "      <td>7.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6081 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Words_per_Sentence  Percentage_First_Person_Singular  \\\n",
       "0              25.066667                          0.000000   \n",
       "1              23.333333                          0.000000   \n",
       "2              24.333333                          0.000000   \n",
       "3              27.466667                          0.404531   \n",
       "4              40.666667                          0.000000   \n",
       "...                  ...                               ...   \n",
       "6076           26.708333                          0.468019   \n",
       "6077           32.700000                          0.000000   \n",
       "6078           25.600000                          0.000000   \n",
       "6079           63.000000                          0.000000   \n",
       "6080           17.125000                          0.729927   \n",
       "\n",
       "      Percentage_Third_Person  Percentage_Exclusive  Percentage_Negation  \\\n",
       "0                    2.393617              1.595745             0.000000   \n",
       "1                    2.857143              2.857143             2.857143   \n",
       "2                    1.369863              1.369863             1.369863   \n",
       "3                    3.559871              0.566343             0.485437   \n",
       "4                    3.278689              0.819672             0.000000   \n",
       "...                       ...                   ...                  ...   \n",
       "6076                 4.680187              2.184087             0.936037   \n",
       "6077                 1.834862              0.305810             0.000000   \n",
       "6078                 4.427083              0.781250             0.520833   \n",
       "6079                 5.555556              2.777778             0.000000   \n",
       "6080                 8.029197              5.109489             0.729927   \n",
       "\n",
       "      Percentage_Causation  Percentage_Sense  Percentage_PositiveEmo  \\\n",
       "0                 2.393617          7.446809                2.393617   \n",
       "1                 2.142857          2.857143                2.142857   \n",
       "2                 0.000000          8.219178                0.000000   \n",
       "3                 1.779935          3.478964                2.831715   \n",
       "4                 3.278689          6.557377                2.459016   \n",
       "...                    ...               ...                     ...   \n",
       "6076              1.560062          1.872075                1.560062   \n",
       "6077              0.000000          4.587156                0.000000   \n",
       "6078              3.125000          2.083333                4.687500   \n",
       "6079              0.793651          2.380952                1.190476   \n",
       "6080              2.919708          5.109489                5.839416   \n",
       "\n",
       "      Percentage_NegativeEmo  Percentage_AffectiveTerms  ...  \\\n",
       "0                   1.861702                   4.255319  ...   \n",
       "1                   0.000000                   2.142857  ...   \n",
       "2                   0.000000                   0.000000  ...   \n",
       "3                   1.699029                   4.530744  ...   \n",
       "4                   2.459016                   4.918033  ...   \n",
       "...                      ...                        ...  ...   \n",
       "6076                2.808112                   4.368175  ...   \n",
       "6077                5.504587                   5.504587  ...   \n",
       "6078                1.562500                   6.250000  ...   \n",
       "6079                1.587302                   2.777778  ...   \n",
       "6080                2.919708                   8.759124  ...   \n",
       "\n",
       "      Percentage_PastVerb  Percentage_PresentVerb  Percentage_FutureVerb  \\\n",
       "0                6.648936                3.191489               0.797872   \n",
       "1                7.857143                1.428571               2.142857   \n",
       "2                4.109589                0.000000               1.369863   \n",
       "3                4.126214                2.993528               0.485437   \n",
       "4                6.557377                3.278689               0.819672   \n",
       "...                   ...                     ...                    ...   \n",
       "6076             3.432137                4.056162               1.092044   \n",
       "6077             1.529052                4.892966               0.000000   \n",
       "6078             1.822917                5.989583               0.781250   \n",
       "6079             2.777778                1.984127               0.793651   \n",
       "6080             2.919708                5.839416               0.000000   \n",
       "\n",
       "      Percentage_Article  Percentage_Pronoun  Percentage_Conjunction  \\\n",
       "0              11.170213            5.585106                5.585106   \n",
       "1               5.000000            9.285714                1.428571   \n",
       "2               6.849315            1.369863                1.369863   \n",
       "3               8.656958            6.877023                4.773463   \n",
       "4               8.196721            4.918033                2.459016   \n",
       "...                  ...                 ...                     ...   \n",
       "6076            4.368175           11.076443                6.240250   \n",
       "6077            9.785933            3.975535                4.892966   \n",
       "6078            6.510417            5.468750                5.989583   \n",
       "6079            8.333333            8.333333                1.984127   \n",
       "6080            7.299270           13.868613                5.839416   \n",
       "\n",
       "      Lexical_Diversity  Emotiveness  Rate_of_Adjectives_Adverbs  \\\n",
       "0              0.486702     0.345133                    0.103723   \n",
       "1              0.742857     0.137255                    0.050000   \n",
       "2              0.698630     0.181818                    0.054795   \n",
       "3              0.411812     0.285360                    0.093042   \n",
       "4              0.672131     0.472222                    0.139344   \n",
       "...                 ...          ...                         ...   \n",
       "6076           0.436817     0.325472                    0.107644   \n",
       "6077           0.483180     0.405405                    0.137615   \n",
       "6078           0.476562     0.297101                    0.106771   \n",
       "6079           0.575397     0.270000                    0.107143   \n",
       "6080           0.671533     0.326531                    0.116788   \n",
       "\n",
       "      Flesch_Kincaid  \n",
       "0                9.9  \n",
       "1               12.0  \n",
       "2                7.7  \n",
       "3               11.6  \n",
       "4               12.4  \n",
       "...              ...  \n",
       "6076            10.1  \n",
       "6077            10.4  \n",
       "6078            11.2  \n",
       "6079            26.6  \n",
       "6080             7.3  \n",
       "\n",
       "[6081 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_linguistic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenization and lowercase\n",
    "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]  # Remove stopwords and lemmatize\n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessed = text.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        beirut ( reuters ) - iran military chief met s...\n",
      "1        hanoi ( reuters ) - top u.s. envoy began two-d...\n",
      "2        ( reuters ) - four u.s. senator asked senate j...\n",
      "3        first read morning briefing meet press nbc pol...\n",
      "4        cairo ( reuters ) - six month egypt election ,...\n",
      "                               ...                        \n",
      "54714    lack oversight prof donald trump totally unfit...\n",
      "54715    tucker carlson responded espn anchor calling p...\n",
      "54716    getting something nothing rage president profe...\n",
      "54717    black emanuelle fixed 1976. attila speaking eu...\n",
      "54718    chaos broke legal american illegal alien clash...\n",
      "Name: text, Length: 54719, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        beirut (reuters) - iran s military chief met w...\n",
      "1        hanoi (reuters) - a top u.s. envoy began a two...\n",
      "2        (reuters) - four u.s. senators have asked the ...\n",
      "3        first read is a morning briefing from meet the...\n",
      "4        cairo (reuters) - six months before egypt s el...\n",
      "                               ...                        \n",
      "54714    this lack of oversight proves that donald trum...\n",
      "54715    tucker carlson responded to an espn anchor cal...\n",
      "54716    because getting something for nothing is all t...\n",
      "54717    black emanuelle fixed all that in 1976. attila...\n",
      "54718    chaos broke out after legal americans and ille...\n",
      "Name: text, Length: 54719, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF vectorizer without specifying max_features\n",
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the preprocessed text data\n",
    "count_matrix = count_vectorizer.fit_transform(text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169079\n"
     ]
    }
   ],
   "source": [
    "# Get the number of unique tokens\n",
    "num_unique_tokens = len(count_vectorizer.get_feature_names_out())\n",
    "print(num_unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize TF-IDF vectorizer with the determined max_features\n",
    "count_vectorizer = CountVectorizer(max_features=num_unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit and transform the text data again with the updated max_features\n",
    "count_matrix = count_vectorizer.fit_transform(text_preprocessed)\n",
    "\n",
    "# Convert the TF-IDF matrix to a CSR (Compressed Sparse Row) matrix for efficient row-wise operations\n",
    "csr_count_matrix = csr_matrix(count_matrix)\n",
    "\n",
    "# Find the row index with the maximum number of filled values\n",
    "max_features_row_index = csr_count_matrix.getnnz(axis=1).argmax()\n",
    "\n",
    "# Get the number of features in the document with the most filled values\n",
    "max_features = csr_count_matrix[max_features_row_index].count_nonzero()\n",
    "\n",
    "svd = TruncatedSVD(n_components=int(max_features*0.3))\n",
    "count_matrix = svd.fit_transform(count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dense_tfidf_matrix = tfidf_matrix[:len(train_data)]\n",
    "#dense_val_tfidf_matrix = tfidf_matrix[len(train_data):len(train_data) + len(val_data)]\n",
    "\n",
    "# Merging the Validation and Training Data into one for a larger training dataset.\n",
    "#dense_count_matrix = count_matrix[:len(train_data) + len(val_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate linguistic features and TF-IDF matrix horizontally\n",
    "dense_count_with_linguistic = hstack([count_matrix, csr_matrix(linguistic_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert them into Arrays\n",
    "train_labels = train_data['label'].values\n",
    "val_labels = val_data['label'].values\n",
    "\n",
    "dense_labels = np.concatenate((train_data['label'].values, val_data['label'].values), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the SVM model\n",
    "svm_model = SVC(kernel='rbf', C=5.0)\n",
    "#svm_model = SVC(kernel='poly', C=20.0, degree=2, coef0=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.95458699 0.95769371 0.95860746 0.95422149 0.96097962]\n",
      "Average CV accuracy: 0.9572178541597529\n"
     ]
    }
   ],
   "source": [
    "#CROSS FOLD VALIDATION, DELETE IF NECESSARY\n",
    "\n",
    "# Initialize KFold with the desired number of folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(svm_model, dense_count_with_linguistic, dense_labels, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Average CV accuracy:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=5.0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=5.0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=5.0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model.fit(dense_count_with_linguistic, dense_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9602039138299622\n"
     ]
    }
   ],
   "source": [
    "text_test_preprocessed = text_test.apply(preprocess_text)\n",
    "test_count_matrix = count_vectorizer.transform(text_test_preprocessed)\n",
    "dense_test_count_matrix = svd.transform(test_count_matrix)\n",
    "dense_test_count_with_linguistic = hstack([dense_test_count_matrix, csr_matrix(test_linguistic_features)])\n",
    "test_labels = test_data['label'].values\n",
    "\n",
    "test_predictions = svm_model.predict(dense_test_count_with_linguistic)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f'Test accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96      3420\n",
      "           1       0.95      0.96      0.95      2661\n",
      "\n",
      "    accuracy                           0.96      6081\n",
      "   macro avg       0.96      0.96      0.96      6081\n",
      "weighted avg       0.96      0.96      0.96      6081\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_labels, test_predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_model_cv.joblib']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "dump(svm_model, 'svm_model_cv.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5839.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy*lengthTestData"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
