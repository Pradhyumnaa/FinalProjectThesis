{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "import liwc\n",
    "import spacy\n",
    "import spacy_transformers\n",
    "import swifter\n",
    "import string\n",
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_data = pd.read_csv('../BaseDataset/train.csv')\n",
    "val_data = pd.read_csv('../BaseDataset/val.csv')\n",
    "test_data = pd.read_csv('../BaseDataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment the code below if it's your first time running this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install en_core_web_md-3.7.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy English model\n",
    "#nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function designed to remove excess whitespaces and numbers. This is to ensure that the tokenize function only counts words.\n",
    "#Numbers do not count towards words as they do not have any associated linguistic features.\n",
    "def preprocess_text(text):\n",
    "\n",
    "    # Characters to keep\n",
    "    characters_to_keep = '.?'\n",
    "\n",
    "    # Additional characters to remove\n",
    "    additional_characters_to_remove = '‘’“”|@#$%^&*(;:),{<>}\"[\\/]+-=_~`'\n",
    "\n",
    "    # Create a translator to remove specified characters\n",
    "    translator = str.maketrans('', '', ''.join(set(string.punctuation) - \n",
    "                                               set(characters_to_keep)) + \n",
    "                                               additional_characters_to_remove)\n",
    "    \n",
    "    # Apply the translation to the text\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "\n",
    "    # Remove excess whitespaces\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer without Spacy\n",
    "def tokenize(text):\n",
    "    for match in re.finditer(r'\\w+', text, re.UNICODE):\n",
    "        yield match.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer to only count words, not punctuations. This for accurate word counting. We're is counted equivalently to We Are.\n",
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" seattle (reuters) - a u.s. federal judge on monday said courtroom proceedings over president donald trump’s travel ban should continue in seattle during an ongoing appeals court review. at a hearing, u.s. district judge james robart in seattle said he was not prepared to slow down the case. robart directed attorneys for the u.s. justice department and washington state’s attorney general to prepare for further proceedings in seattle.\"\n",
    "text = preprocess_text(text)\n",
    "doc = nlp(text)\n",
    "print(text)\n",
    "\n",
    "print(textstat.flesch_kincaid_grade(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse, category_names = liwc.load_token_parser('LIWC2007_English100131.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linguistic Features based on Gravanis\n",
    "def calculate_linguistic_features(text):\n",
    "    text = preprocess_text(text)\n",
    "    doc = nlp(text)\n",
    "    total_words = len(tokenize(text))\n",
    "    total_sentences = len(list(doc.sents))\n",
    "\n",
    "    # List of third person pronouns\n",
    "    third_person_words = [\"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\"]\n",
    "\n",
    "    # Initialize counts\n",
    "    third_person_count = 0\n",
    "\n",
    "    # Set to store unique words\n",
    "    unique_words = set()\n",
    "\n",
    "    # Count occurrences of pronouns and other linguistic features\n",
    "    text_tokens = tokenize(text)\n",
    "    token_counts = Counter(category for token in text_tokens \n",
    "                           for category in parse(token))\n",
    "\n",
    "    for token in doc:\n",
    "        if token.lower_ in third_person_words:\n",
    "            third_person_count += 1\n",
    "        if token.is_alpha:\n",
    "            unique_words.add(token.text.lower())\n",
    "\n",
    "    # Count the number of adjectives, adverbs, nouns, and verbs\n",
    "    adjectives = sum(1 for token in doc if token.pos_ == \"ADJ\")\n",
    "    adverbs = sum(1 for token in doc if token.pos_ == \"ADV\")\n",
    "    nouns = sum(1 for token in doc if token.pos_ == \"NOUN\")\n",
    "    verbs = sum(1 for token in doc if token.pos_ == \"VERB\")\n",
    "\n",
    "    # Calculate emotiveness using the formula\n",
    "    emotiveness = (adjectives + adverbs) / max((nouns + verbs), 1) if nouns + verbs > 0 else 0\n",
    "\n",
    "    # Rate of Adjectives and Adverbs\n",
    "    rate_of_adj = (adjectives + adverbs) / max((total_words), 1) if total_words > 0 else 0\n",
    "\n",
    "    #Lexical Diversity\n",
    "    lexical_diversity = (len(unique_words)/ total_words) if total_words > 0 else 0\n",
    "\n",
    "    # Retrieve counts from the LIWC dictionary\n",
    "    first_person_singular_count = token_counts['i']\n",
    "    exclusive_count = token_counts['excl']\n",
    "    negation_count = token_counts['negate']\n",
    "    causation_count = token_counts['cause']\n",
    "    senses_count = token_counts['percept'] + token_counts['see'] + token_counts['hear'] + token_counts['feel']\n",
    "    positive_emotions = token_counts['posemo']\n",
    "    negative_emotions = token_counts['negemo']\n",
    "    affective_terms = token_counts['affect']\n",
    "    prepositions = token_counts['preps']\n",
    "    cognitive_processes = token_counts['cogmech']\n",
    "    insight_count = token_counts['insight']\n",
    "    discrepancy_count = token_counts['discrep']\n",
    "    tentative_count = token_counts['tentat']\n",
    "    certainty_count = token_counts['certain']\n",
    "    social_count = token_counts['social']\n",
    "    space_count = token_counts['space']\n",
    "    inclusive_count = token_counts['incl']\n",
    "    motion_count = token_counts['motion']\n",
    "    time_count = token_counts['time']\n",
    "    past_verb = token_counts['past']\n",
    "    present_verb = token_counts['present']\n",
    "    future_verb = token_counts['future']\n",
    "    article_count = token_counts['article']\n",
    "    pronoun_count = token_counts['pronoun']\n",
    "    conjunction_count = token_counts['conj']\n",
    "    flesch_kincaid = textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "    return {\n",
    "        'Words_per_Sentence': total_words / total_sentences if total_sentences > 0 else 0,\n",
    "        'Percentage_First_Person_Singular': (first_person_singular_count / total_words) * 100 if total_words > 0 else 0,\n",
    "        'Percentage_Third_Person': (third_person_count / total_words) * 100 if total_words > 0 else 0,\n",
    "        'Percentage_Exclusive': (exclusive_count / total_words) * 100 if total_words > 0 else 0,\n",
    "        'Percentage_Negation': (negation_count / total_words) * 100 if total_words > 0 else 0,\n",
    "        'Percentage_Causation': (causation_count / total_words) * 100 if total_words > 0 else 0,\n",
    "        'Percentage_Sense': (senses_count / total_words) * 100 if total_words > 0 else 0,\n",
    "        'Percentage_PositiveEmo': (positive_emotions / total_words) * 100 if total_words > 0 else 0,\n",
    "        'Percentage_NegativeEmo': (negative_emotions / total_words) * 100 if total_words > 0 else 0,\n",
    "        'Percentage_AffectiveTerms': (affective_terms / total_words) * 100 if total_words > 0 else 0,\n",
    "        'Percentage_Prepositions': (prepositions / total_words) * 100 if total_words > 0 else 0,\n",
    "        'Percentage_CognitiveProcess': (cognitive_processes / total_words) * 100 if total_words > 0 else 0,\n",
    "        'Percentage_Insight': (insight_count / total_words) * 100 if total_words > 0 else 0,\n",
    "        'Percentage_Discrepancy': (discrepancy_count / total_words) * 100 if total_words > 0 else 0,\n",
    "        'Percentage_Tentative': (tentative_count / total_words) * 100 if total_words > 0 else 0,\n",
    "        'Percentage_Certainty': (certainty_count / total_words) * 100 if total_words > 0 else 0,\n",
    "        'Percentage_Social': (social_count / total_words) * 100 if total_words > 0 else 0,\n",
    "        'Percentage_Space': (space_count / total_words) * 100 if total_words > 0 else 0,\n",
    "        'Percentage_Inclusive': (inclusive_count / total_words) * 100 if total_words > 0 else 0,\n",
    "        'Percentage_Motion': (motion_count / total_words) * 100 if total_words > 0 else 0,\n",
    "        'Percentage_Time': (time_count / total_words) * 100 if total_words > 0 else 0,\n",
    "        'Percentage_PastVerb': (past_verb / total_words) * 100 if total_words > 0 else 0,\n",
    "        'Percentage_PresentVerb': (present_verb / total_words) * 100 if total_words > 0 else 0,\n",
    "        'Percentage_FutureVerb': (future_verb / total_words) * 100 if total_words > 0 else 0,\n",
    "        'Percentage_Article': (article_count / total_words) * 100 if total_words > 0 else 0,\n",
    "        'Percentage_Pronoun': (pronoun_count / total_words) * 100 if total_words > 0 else 0,\n",
    "        'Percentage_Conjunction': (conjunction_count / total_words) * 100 if total_words > 0 else 0,\n",
    "        'Lexical_Diversity': (lexical_diversity),\n",
    "        'Emotiveness': (emotiveness),\n",
    "        'Rate_of_Adjectives_Adverbs': (rate_of_adj),\n",
    "        'Flesch_Kincaid': (flesch_kincaid),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to val_data in parallel\n",
    "val_data = pd.concat([val_data, val_data['text'].swifter.apply(calculate_linguistic_features).apply(pd.Series)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.to_csv('val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to test_data in parallel\n",
    "test_data = pd.concat([test_data, test_data['text'].swifter.apply(calculate_linguistic_features).apply(pd.Series)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to train_data in parallel\n",
    "train_data = pd.concat([train_data, train_data['text'].swifter.apply(calculate_linguistic_features).apply(pd.Series)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.value_counts(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.value_counts(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.value_counts(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files into DataFrames\n",
    "train_data = pd.read_csv('train.csv')\n",
    "val_data = pd.read_csv('val.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Get unique IDs for each dataset\n",
    "train_unique_ids = set(train_data['unique_id'])\n",
    "val_unique_ids = set(val_data['unique_id'])\n",
    "test_unique_ids = set(test_data['unique_id'])\n",
    "\n",
    "# Find unique IDs in train not in val or test\n",
    "train_not_in_val = train_unique_ids - val_unique_ids\n",
    "train_not_in_test = train_unique_ids - test_unique_ids\n",
    "\n",
    "# Find unique IDs in val not in train or test\n",
    "val_not_in_train = val_unique_ids - train_unique_ids\n",
    "val_not_in_test = val_unique_ids - test_unique_ids\n",
    "\n",
    "# Find unique IDs in test not in train or val\n",
    "test_not_in_train = test_unique_ids - train_unique_ids\n",
    "test_not_in_val = test_unique_ids - val_unique_ids\n",
    "\n",
    "print(f\"Rows in train_data: {len(train_data)}\")\n",
    "print(f\"Rows in train_data not in val_data: {len(train_not_in_val)}\")\n",
    "print(f\"Rows in train_data not in test_data: {len(train_not_in_test)}\")\n",
    "print(\"\")\n",
    "\n",
    "print(f\"Rows in train_data: {len(val_data)}\")\n",
    "print(f\"Rows in val_data not in train_data: {len(val_not_in_train)}\")\n",
    "print(f\"Rows in val_data not in test_data: {len(val_not_in_test)}\")\n",
    "print(\"\")\n",
    "\n",
    "print(f\"Rows in test_data: {len(test_data)}\")\n",
    "print(f\"Rows in test_data not in train_data: {len(test_not_in_train)}\")\n",
    "print(f\"Rows in test_data not in val_data: {len(test_not_in_val)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
